{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497db21d-4e08-42a7-9258-90fecced929f",
   "metadata": {
    "id": "497db21d-4e08-42a7-9258-90fecced929f"
   },
   "source": [
    "# L4: Training a Dual Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4cf2e4-def8-44c7-a5f2-cf88836e9e22",
   "metadata": {
    "id": "3d4cf2e4-def8-44c7-a5f2-cf88836e9e22"
   },
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(Kernel Starting)</code>:</b> This notebook takes about 30 seconds to be ready to use. You may start and watch the video while you wait.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5725255c-8e4e-459b-ab12-29fca393b662",
   "metadata": {
    "height": 64,
    "id": "5725255c-8e4e-459b-ab12-29fca393b662"
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4",
   "metadata": {
    "height": 98,
    "id": "23226224-1ca8-4ab3-b8b6-ffd560b205e4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6fc6d7-218b-44a3-ac58-6ef41b5faf8f",
   "metadata": {
    "id": "5e6fc6d7-218b-44a3-ac58-6ef41b5faf8f"
   },
   "source": [
    "<p style=\"background-color:#fff6ff; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> üíª &nbsp; <b>Access <code>requirements.txt</code> file:</b> To access <code>requirements.txt</code> for this notebook, 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix - Tips and Help\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce87a1-5ca5-41ec-b7d2-e44a892f7b9a",
   "metadata": {
    "id": "b1ce87a1-5ca5-41ec-b7d2-e44a892f7b9a"
   },
   "source": [
    "## The CrossEntropyLoss 'trick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
   "metadata": {
    "height": 166,
    "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        [4.3, 1.2, 0.05, 1.07],\n",
    "        [0.18, 3.2, 0.09, 0.05],\n",
    "        [0.85, 0.27, 2.2, 1.03],\n",
    "        [0.23, 0.57, 0.12, 5.1]\n",
    "    ]\n",
    ")\n",
    "data = torch.tensor(df.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916a47b2-ece3-4719-a8bb-162f3a1fd33d",
   "metadata": {
    "height": 81,
    "id": "916a47b2-ece3-4719-a8bb-162f3a1fd33d"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(data):\n",
    "    target = torch.arange(data.size(0))\n",
    "    loss = torch.nn.CrossEntropyLoss()(data, target)\n",
    "    return loss\n",
    "\n",
    "# given the data, compute the loss given contrastive loss.\n",
    "\n",
    "# cross enthropy --> softmax involved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
    "outputId": "364958b5-f19d-4898-fa57-44180ef25d2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9100, 0.0410, 0.0130, 0.0360],\n",
       "        [0.0429, 0.8801, 0.0393, 0.0377],\n",
       "        [0.1512, 0.0846, 0.5832, 0.1810],\n",
       "        [0.0075, 0.0105, 0.0067, 0.9753]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
    "outputId": "25224a38-a989-48fa-9c2f-3db687b4d350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(dim=1)(data).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bc91c4-fe9f-44d7-8735-c554ec3c4bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 183,
    "id": "83bc91c4-fe9f-44d7-8735-c554ec3c4bfd",
    "outputId": "9b75b8c5-18a4-4863-b6cd-55e8e0d57aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3000, 1.2000, 0.0500, 1.0700],\n",
      "        [0.1800, 3.2000, 0.0900, 0.0500],\n",
      "        [0.8500, 0.2700, 2.2000, 1.0300],\n",
      "        [0.2300, 0.5700, 0.1200, 5.1000]])\n",
      "Loss = 0.19657586514949799\n",
      "tensor([[4.8000, 1.1800, 0.0300, 1.0500],\n",
      "        [0.1600, 3.7000, 0.0700, 0.0300],\n",
      "        [0.8300, 0.2500, 2.7000, 1.0100],\n",
      "        [0.2100, 0.5500, 0.1000, 5.6000]])\n",
      "Loss = 0.12602083384990692\n",
      "tensor([[5.3000, 1.1600, 0.0100, 1.0300],\n",
      "        [0.1400, 4.2000, 0.0500, 0.0100],\n",
      "        [0.8100, 0.2300, 3.2000, 0.9900],\n",
      "        [0.1900, 0.5300, 0.0800, 6.1000]])\n",
      "Loss = 0.07888662070035934\n"
     ]
    }
   ],
   "source": [
    "N = data.size(0)\n",
    "non_diag_mask = ~torch.eye(N, N, dtype=bool)\n",
    "\n",
    "for inx in range(3):\n",
    "    data = torch.tensor(df.values, dtype=torch.float32)\n",
    "    data[range(N), range(N)] += inx*0.5\n",
    "    # so you increase the value diagonally, but decrease the value elsewhere.\n",
    "    # what happens is diagnol grows but the rest decrease. every iteration\n",
    "    # loss keeps reducing every iteration.\n",
    "    data[non_diag_mask] -= inx*0.02\n",
    "    print(data)\n",
    "    print(f\"Loss = {contrastive_loss(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d4e24-3cb2-416b-8705-9d28e86a6b86",
   "metadata": {
    "id": "bc5d4e24-3cb2-416b-8705-9d28e86a6b86"
   },
   "source": [
    "## The Encoder module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3254ee3-4aef-4dca-802f-334275a06d05",
   "metadata": {
    "height": 302,
    "id": "b3254ee3-4aef-4dca-802f-334275a06d05"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_embed_dim):\n",
    "        super().__init__()\n",
    "        # batch size 32, output 128\n",
    "        # nn embedding model, integers --> token embeddings\n",
    "        # 32 X seqLen\n",
    "        # 32 X seqLeng X512\n",
    "        # the embeddings output 128\n",
    "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(embed_dim, nhead=8, batch_first=True),\n",
    "            num_layers=3,\n",
    "            norm=torch.nn.LayerNorm([embed_dim]),\n",
    "            enable_nested_tensor=False\n",
    "        )\n",
    "        self.projection = torch.nn.Linear(embed_dim, output_embed_dim)\n",
    "\n",
    "    def forward(self, tokenizer_output):\n",
    "        x = self.embedding_layer(tokenizer_output['input_ids'])\n",
    "        x = self.encoder(x, src_key_padding_mask=tokenizer_output['attention_mask'].logical_not())\n",
    "        cls_embed = x[:,0,:]\n",
    "        return self.projection(cls_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d2b75-1e6b-4a25-ba17-adcca6f1b11f",
   "metadata": {
    "id": "435d2b75-1e6b-4a25-ba17-adcca6f1b11f"
   },
   "source": [
    "![Diagram 1](DLAI-diagram-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7195b4-d7d2-4bf6-850f-a7793afca09b",
   "metadata": {
    "id": "cc7195b4-d7d2-4bf6-850f-a7793afca09b"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
   "metadata": {
    "height": 812,
    "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataset):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size,\n",
    "                               output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size,\n",
    "                             output_embed_size)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                             shuffle=True)\n",
    "    # cross entropy\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(question_encoder.parameters()) + list(answer_encoder.parameters()\n",
    "    ), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    running_loss = []\n",
    "    for _, data_batch in enumerate(dataloader):\n",
    "\n",
    "        # Tokenize the question/answer pairs (each is a batch of 32 questions and 32 answers)\n",
    "        question, answer = data_batch\n",
    "        question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "        answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "\n",
    "        # Compute the embeddings: the output is of dim = 32 x 128\n",
    "        question_embed = question_encoder(question_tok)\n",
    "        answer_embed = answer_encoder(answer_tok)\n",
    "\n",
    "        # Compute similarity scores: a 32x32 matrix\n",
    "        # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "        similarity_scores = question_embed @ answer_embed.T\n",
    "\n",
    "        # we want to maximize the values in the diagonal\n",
    "        target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "        loss = loss_fn(similarity_scores, target)\n",
    "        running_loss += [loss.item()]\n",
    "\n",
    "        # this is where the magic happens\n",
    "        optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f33a1-6615-460c-af10-17aaa63b7797",
   "metadata": {
    "id": "c61f33a1-6615-460c-af10-17aaa63b7797"
   },
   "source": [
    "## Training in multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff76b1a9-02f3-4090-a1cb-e494cbe7ea8f",
   "metadata": {
    "height": 931,
    "id": "ff76b1a9-02f3-4090-a1cb-e494cbe7ea8f"
   },
   "outputs": [],
   "source": [
    "# same setup, but make it a function so we can reuse it\n",
    "\n",
    "def train(dataset, num_epochs=10):\n",
    "    embed_size = 512\n",
    "    output_embed_size = 128\n",
    "    max_seq_len = 64\n",
    "    batch_size = 32\n",
    "\n",
    "    n_iters = len(dataset) // batch_size + 1\n",
    "\n",
    "    # define the question/answer encoders\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    question_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "    answer_encoder = Encoder(tokenizer.vocab_size, embed_size, output_embed_size)\n",
    "\n",
    "    # define the dataloader, optimizer and loss function\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(list(question_encoder.parameters()) + list(answer_encoder.parameters()), lr=1e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = []\n",
    "        for idx, data_batch in enumerate(dataloader):\n",
    "\n",
    "            # Tokenize the question/answer pairs (each is a batc of 32 questions and 32 answers)\n",
    "            question, answer = data_batch\n",
    "            question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            answer_tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=max_seq_len)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_tok['input_ids'].shape, answer_tok['input_ids'].shape)\n",
    "\n",
    "            # Compute the embeddings: the output is of dim = 32 x 128\n",
    "            question_embed = question_encoder(question_tok)\n",
    "            answer_embed = answer_encoder(answer_tok)\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(question_embed.shape, answer_embed.shape)\n",
    "\n",
    "            # Compute similarity scores: a 32x32 matrix\n",
    "            # row[N] reflects similarity between question[N] and answers[0...31]\n",
    "            similarity_scores = question_embed @ answer_embed.T\n",
    "            if inx == 0 and epoch == 0:\n",
    "                print(similarity_scores.shape)\n",
    "\n",
    "            # we want to maximize the values in the diagonal\n",
    "            target = torch.arange(question_embed.shape[0], dtype=torch.long)\n",
    "            loss = loss_fn(similarity_scores, target)\n",
    "            running_loss += [loss.item()]\n",
    "            if idx == n_iters-1:\n",
    "                print(f\"Epoch {epoch}, loss = \", np.mean(running_loss))\n",
    "\n",
    "            # this is where the magic happens\n",
    "            optimizer.zero_grad()    # reset optimizer so gradients are all-zero\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return question_encoder, answer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ce22f-b921-49af-b932-16e9926f945c",
   "metadata": {
    "id": "918ce22f-b921-49af-b932-16e9926f945c"
   },
   "source": [
    "## Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9733e9a-2704-4bb1-b7ff-0d6c710d688a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "height": 217,
    "id": "b9733e9a-2704-4bb1-b7ff-0d6c710d688a",
    "outputId": "ca913fd0-7f33-4f88-d9b9-6a506e04d909"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"questions\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"where did the 2017 tour de france start\",\n          \"what do you need to be an ontario scholar\",\n          \"who is the chess champion of the world\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The 3,540 km (2,200 mi)-long race commenced with an individual time trial in D\\u00c3\\u00bcsseldorf, Germany on 1 July, and concluded with the Champs-\\u00c3\\u2030lys\\u00c3\\u00a9es stage in Paris on 23 July.\",\n          \"Ontario Scholars are high school graduates in the Canadian province of Ontario who attain an average of 80 % or greater in their six best Grade 12 courses.\",\n          \"Current world champion Magnus Carlsen won the World Chess Championship 2013 against Viswanathan Anand and successfully defended his title against Anand in the World Chess Championship 2014 and against Sergey Karjakin in the World Chess Championship 2016.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d4333206-5e24-4029-b3cc-521e63c74695\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who played bubba in the tv series in the heat ...</td>\n",
       "      <td>Carlos Alan Autry Jr. (also known for a period...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where did the 2017 tour de france start</td>\n",
       "      <td>The 3,540 km (2,200 mi)-long race commenced wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who is the chess champion of the world</td>\n",
       "      <td>Current world champion Magnus Carlsen won the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who scored the most hat tricks in football</td>\n",
       "      <td>Cristiano Ronaldo and Messi have scored three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what do you need to be an ontario scholar</td>\n",
       "      <td>Ontario Scholars are high school graduates in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d4333206-5e24-4029-b3cc-521e63c74695')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d4333206-5e24-4029-b3cc-521e63c74695 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d4333206-5e24-4029-b3cc-521e63c74695');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-952b4f4e-a947-4315-8f68-3e17ed0143bd\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-952b4f4e-a947-4315-8f68-3e17ed0143bd')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-952b4f4e-a947-4315-8f68-3e17ed0143bd button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0  who played bubba in the tv series in the heat ...   \n",
       "1            where did the 2017 tour de france start   \n",
       "2             who is the chess champion of the world   \n",
       "3         who scored the most hat tricks in football   \n",
       "4          what do you need to be an ontario scholar   \n",
       "\n",
       "                                             answers  \n",
       "0  Carlos Alan Autry Jr. (also known for a period...  \n",
       "1  The 3,540 km (2,200 mi)-long race commenced wi...  \n",
       "2  Current world champion Magnus Carlsen won the ...  \n",
       "3  Cristiano Ronaldo and Messi have scored three ...  \n",
       "4  Ontario Scholars are high school graduates in ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer and question pairs\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        self.data = pd.read_csv(datapath, sep=\"\\t\", nrows=300)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]['questions'], self.data.iloc[idx]['answers']\n",
    "\n",
    "# we just chose this dataset\n",
    "dataset = MyDataset('./shared_data/nq_sample.tsv')\n",
    "dataset.data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb3d80-8635-4800-83c2-ca64991b98dd",
   "metadata": {
    "id": "e3bb3d80-8635-4800-83c2-ca64991b98dd"
   },
   "source": [
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ‚è≥ <b>Note <code>(num_epochs = 5)</code>:</b> The <code>num_epochs</code> is set to <code>5</code> for speedier execution. You may train the model\n",
    "using a higher number of epochs by changing this parameter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eca1874-c660-4336-b411-5ab470b0a9b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "7eca1874-c660-4336-b411-5ab470b0a9b5",
    "outputId": "955c0f6e-c783-493b-c78c-b1701348fa48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss =  3.685177206993103\n",
      "Epoch 1, loss =  3.559368062019348\n",
      "Epoch 2, loss =  3.3996769428253173\n",
      "Epoch 3, loss =  3.3536009550094605\n",
      "Epoch 4, loss =  3.3046725034713744\n",
      "Epoch 5, loss =  3.217980480194092\n",
      "Epoch 6, loss =  3.2009748220443726\n",
      "Epoch 7, loss =  3.1478068351745607\n",
      "Epoch 8, loss =  3.104023313522339\n",
      "Epoch 9, loss =  2.9822537660598756\n",
      "Epoch 10, loss =  2.9273232936859133\n",
      "Epoch 11, loss =  2.851041853427887\n",
      "Epoch 12, loss =  2.7698217630386353\n",
      "Epoch 13, loss =  2.6535635113716127\n",
      "Epoch 14, loss =  2.489632260799408\n",
      "Epoch 15, loss =  2.4132778644561768\n",
      "Epoch 16, loss =  2.3328142523765565\n",
      "Epoch 17, loss =  2.2107775092124937\n",
      "Epoch 18, loss =  1.9972907423973083\n",
      "Epoch 19, loss =  1.9028201580047608\n",
      "Epoch 20, loss =  1.6866387724876404\n",
      "Epoch 21, loss =  1.5735480070114136\n",
      "Epoch 22, loss =  1.443335384130478\n",
      "Epoch 23, loss =  1.2469069123268128\n",
      "Epoch 24, loss =  1.03388711810112\n",
      "Epoch 25, loss =  0.9558795899152756\n",
      "Epoch 26, loss =  0.8536964371800423\n",
      "Epoch 27, loss =  0.7203091889619827\n",
      "Epoch 28, loss =  0.6300518691539765\n",
      "Epoch 29, loss =  0.5318085871636867\n",
      "Epoch 30, loss =  0.47019428536295893\n",
      "Epoch 31, loss =  0.3915229246020317\n",
      "Epoch 32, loss =  0.3408074676990509\n",
      "Epoch 33, loss =  0.29582519456744194\n",
      "Epoch 34, loss =  0.27998757883906367\n",
      "Epoch 35, loss =  0.2534236997365952\n",
      "Epoch 36, loss =  0.21991848517209292\n",
      "Epoch 37, loss =  0.20024867206811905\n",
      "Epoch 38, loss =  0.15946313962340355\n",
      "Epoch 39, loss =  0.15865560956299304\n",
      "Epoch 40, loss =  0.14544619023799896\n",
      "Epoch 41, loss =  0.11237235702574253\n",
      "Epoch 42, loss =  0.12058103680610657\n",
      "Epoch 43, loss =  0.10040440065786242\n",
      "Epoch 44, loss =  0.11051380373537541\n",
      "Epoch 45, loss =  0.10243515390902758\n",
      "Epoch 46, loss =  0.07549301460385323\n",
      "Epoch 47, loss =  0.09778462871909141\n",
      "Epoch 48, loss =  0.07176293982192875\n",
      "Epoch 49, loss =  0.0633135100826621\n"
     ]
    }
   ],
   "source": [
    "qe, ae = train(dataset, num_epochs=50)\n",
    "\n",
    "# just show, could be very larg epochs.\n",
    "# with 50 epochs, you still do not get the correct answer\n",
    "# loss from 3.685177206993103 to 0.0633135100826621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29c17cfc-02ad-4a02-99fc-da55da8c17ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 132,
    "id": "29c17cfc-02ad-4a02-99fc-da55da8c17ec",
    "outputId": "db8e5ff4-9c0e-4287-b24f-37a5f2323313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([-1.4179,  0.3797,  0.1785,  0.0984,  0.1167], grad_fn=<SliceBackward0>)\n",
      "What is the tallest mountain in the world?\n"
     ]
    }
   ],
   "source": [
    "question = 'What is the tallest mountain in the world?'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "question_emb = qe(question_tok)[0]\n",
    "print(question_tok)\n",
    "print(question_emb[:5])\n",
    "print(question)\n",
    "# just to see the input integers and embeddings would be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a33357-6507-4ae9-a638-2de4af933c86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 302,
    "id": "f7a33357-6507-4ae9-a638-2de4af933c86",
    "outputId": "436092e6-2411-49d2-adff-51b6768a6a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2054,  2003,  1996, 13747,  3137,  1999,  1996,  2088,  1029,\n",
      "           102]]), tensor([[  101,  1996, 13747,  3137,  1999,  1996,  2088,  2003,  4057, 23914,\n",
      "          1012,   102]]), tensor([[ 101, 2040, 2003, 6221, 9457, 1029,  102]])]\n",
      "tensor([-0.1007, -0.7156, -0.5800,  0.5016, -1.0491], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.0444, -0.9409, -0.4757,  0.2325, -0.6030], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.3486, -0.1412, -1.0564,  0.8505, -0.3759], grad_fn=<SliceBackward0>)\n",
      "Who is donald duck?\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    \"What is the tallest mountain in the world?\",\n",
    "    \"The tallest mountain in the world is Mount Everest.\",\n",
    "    \"Who is donald duck?\"\n",
    "]\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = ae(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af5a9a16-016c-4240-9a3f-847c5a7d5624",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "af5a9a16-016c-4240-9a3f-847c5a7d5624",
    "outputId": "f67d891d-837a-4518-fb63-533d8e0f4f25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.8581,  1.7138, -2.3971], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_emb @ torch.stack(answer_emb).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "789b0aa3-7a3b-4618-bc7e-14d18492a42d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "789b0aa3-7a3b-4618-bc7e-14d18492a42d",
    "outputId": "cf218798-b174-4294-8c72-4fe681a338bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2054,  2003,  1996,  2190,  2828,  1997, 27130,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([-0.1385,  0.5270, -0.1360,  0.0173,  0.4712], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "question = 'What is the best type of noodles?'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "question_tok = tokenizer(question, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "question_emb = qe(question_tok)[0]\n",
    "print(question_tok)\n",
    "print(question_emb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd7fd33c-c5f2-466a-b011-b6dc8495c2be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "fd7fd33c-c5f2-466a-b011-b6dc8495c2be",
    "outputId": "8497531a-4028-44b8-a5f7-91dc6e2389b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101,  2054,  2003,  1996,  2190,  2828,  1997, 27130,  1029,   102]]), tensor([[  101,  1996,  2190,  2828,  1997,  2053, 26156,  2003, 20919,  2053,\n",
      "         26156,   102]]), tensor([[ 101, 2040, 2003, 6221, 9457, 1029,  102]])]\n",
      "tensor([-0.4858, -0.2156, -0.9164,  0.7267, -0.1336], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.6265,  0.0294, -0.6781,  0.5177, -0.0026], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.1988,  0.1506, -1.0794,  0.7419, -0.6434], grad_fn=<SliceBackward0>)\n",
      "Who is donald duck?\n"
     ]
    }
   ],
   "source": [
    "answers = [\n",
    "    'What is the best type of noodles?',\n",
    "    \"the best type of noodle is jelly noodle\",\n",
    "    \"Who is donald duck?\"\n",
    "]\n",
    "answer_tok = []\n",
    "answer_emb = []\n",
    "for answer in answers:\n",
    "    tok = tokenizer(answer, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    answer_tok.append(tok['input_ids'])\n",
    "    emb = ae(tok)[0]\n",
    "    answer_emb.append(emb)\n",
    "\n",
    "print(answer_tok)\n",
    "print(answer_emb[0][:5])\n",
    "print(answer_emb[1][:5])\n",
    "print(answer_emb[2][:5])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d282a2-0a68-4ef1-baab-15a1ec809741",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "80d282a2-0a68-4ef1-baab-15a1ec809741",
    "outputId": "086fedd9-a0aa-4dfd-fb8c-15ee84b2d69b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.2859, 14.3123,  8.8230], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_emb @ torch.stack(answer_emb).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3doQPrudsKZZ",
   "metadata": {
    "id": "3doQPrudsKZZ"
   },
   "source": [
    "# the training is built through PyTorch modules,\n",
    "# and we see how the training develops\n",
    "# one example with small example.\n",
    "# how this works well with full trained algo that can work with RAG in next lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2243d-0984-42ac-92a1-81a20dd4df2d",
   "metadata": {
    "height": 30,
    "id": "ffd2243d-0984-42ac-92a1-81a20dd4df2d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdc3b8-b5cc-40ec-83a0-d4c56fd481b4",
   "metadata": {
    "height": 30,
    "id": "edbdc3b8-b5cc-40ec-83a0-d4c56fd481b4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
