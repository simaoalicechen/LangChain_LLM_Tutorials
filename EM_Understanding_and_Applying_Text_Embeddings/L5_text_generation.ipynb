{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f357d1-bbc9-4848-828c-00b36144ebdc",
   "metadata": {
    "id": "e4f357d1-bbc9-4848-828c-00b36144ebdc"
   },
   "source": [
    "## Lesson 5: Text Generation with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416b662-c645-4048-8557-fd0f66d9818c",
   "metadata": {
    "id": "a416b662-c645-4048-8557-fd0f66d9818c"
   },
   "source": [
    "#### Project environment setup\n",
    "\n",
    "- Load credentials and relevant Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "w0hrsUrkcfQK",
   "metadata": {
    "id": "w0hrsUrkcfQK"
   },
   "outputs": [],
   "source": [
    "# !pip install mplcursors\n",
    "# !pip install google-cloud-bigquery google-auth\n",
    "# !pip install python-dotenv\n",
    "# !pip install ipympl\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9d7e7fc-0c16-41c5-ba3d-988fb011deb5",
   "metadata": {
    "id": "a9d7e7fc-0c16-41c5-ba3d-988fb011deb5"
   },
   "outputs": [],
   "source": [
    "from utils_5 import authenticate\n",
    "credentials, PROJECT_ID = authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff77bf9c-93e6-4281-9775-f7e87935b37a",
   "metadata": {
    "id": "ff77bf9c-93e6-4281-9775-f7e87935b37a"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fb2abd-f5d3-4fa0-8c4c-8d516df20c7f",
   "metadata": {
    "id": "02fb2abd-f5d3-4fa0-8c4c-8d516df20c7f"
   },
   "source": [
    "### Prompt the model\n",
    "- We'll import a language model that has been trained to handle a variety of natural language tasks, `text-bison@001`.\n",
    "- For multi-turn dialogue with a language model, you can use, `chat-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc8793c-bae4-405e-9dcc-2d5657c7486b",
   "metadata": {
    "id": "4bc8793c-bae4-405e-9dcc-2d5657c7486b"
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=REGION,\n",
    "              credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f253273a-a0bd-41c8-bf64-138666578504",
   "metadata": {
    "id": "f253273a-a0bd-41c8-bf64-138666578504"
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-cWUOo0ldXYc",
   "metadata": {
    "id": "-cWUOo0ldXYc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# API stuff:\n",
    "# Do not forget to erase\n",
    "project = os.env.get[\"PROJECT_ID\"]\n",
    "REGION = 'us-central1'\n",
    "# project = project\n",
    "credentials = os.env.get[\"credentials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "WT5rn-A_dmaW",
   "metadata": {
    "id": "WT5rn-A_dmaW"
   },
   "outputs": [],
   "source": [
    "# have to add this part back too. Important\n",
    "# from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Load the credentials explicitly\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'google_api.json')\n",
    "\n",
    "# Initialize Vertex AI with the correct credentials\n",
    "aiplatform.init(project= project , location=\"us-central1\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da149416-7fea-4bb3-b872-f64200f9b19c",
   "metadata": {
    "id": "da149416-7fea-4bb3-b872-f64200f9b19c"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\n",
    "    \"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VC5S1blmfCbk",
   "metadata": {
    "id": "VC5S1blmfCbk"
   },
   "source": [
    "## get the api stuff, and get the embedding models (pretrained here)\n",
    "## now, send a promt and see how the embedding models can generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Elx1hKqXfB1y",
   "metadata": {
    "id": "Elx1hKqXfB1y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ac4f25-7818-49fc-b9e9-097886da8e82",
   "metadata": {
    "id": "36ac4f25-7818-49fc-b9e9-097886da8e82"
   },
   "source": [
    "#### Question Answering\n",
    "- You can ask an open-ended question to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3a1b5ab-1bb8-4882-93e8-cc9381b18aad",
   "metadata": {
    "id": "c3a1b5ab-1bb8-4882-93e8-cc9381b18aad"
   },
   "outputs": [],
   "source": [
    "prompt = \"I'm a high school student. \\\n",
    "Recommend me a programming activity to improve my skills.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84aca913-e2c1-4ad5-b8ee-4945c9546102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84aca913-e2c1-4ad5-b8ee-4945c9546102",
    "outputId": "4c080ba1-5f35-4ecd-c7fc-6fe101f3ad09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **Write a program to solve a problem you're interested in.** This could be anything from a game to a tool to help you with your studies. The important thing is that you're interested in the problem and that you're motivated to solve it.\n",
      "* **Take a programming course.** There are many online and offline courses available, so you can find one that fits your schedule and learning style.\n",
      "* **Join a programming community.** There are many online and offline communities where you can connect with other programmers and learn from each other.\n",
      "* **Read programming books and articles.** There is a\n"
     ]
    }
   ],
   "source": [
    "print(generation_model.predict(prompt=prompt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd540471-cda7-43b2-aee3-b84cb1234c1a",
   "metadata": {
    "id": "fd540471-cda7-43b2-aee3-b84cb1234c1a"
   },
   "source": [
    "#### Classify and elaborate\n",
    "- For more predictability of the language model's response, you can also ask the language model to choose among a list of answers and then elaborate on its answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1491242-77fb-4d8e-8bda-04f4894fdd83",
   "metadata": {
    "id": "e1491242-77fb-4d8e-8bda-04f4894fdd83"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"I'm a high school student. \\\n",
    "Which of these activities do you suggest and why:\n",
    "a) learn Python\n",
    "b) learn Javascript\n",
    "c) learn Fortran\n",
    "\"\"\"\n",
    "# the maximum token they would generate is 300 :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28880988-e191-4991-baa8-fb7c89cc4874",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28880988-e191-4991-baa8-fb7c89cc4874",
    "outputId": "c4577945-36ea-4460-abcf-5aa5e9ba6f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As I prepared the picture frame, I reached into my toolkit to fetch my hammer.\n"
     ]
    }
   ],
   "source": [
    "print(generation_model.predict(prompt=prompt).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e102f-e4d7-4ac8-9b76-3a0c1b3790b9",
   "metadata": {
    "id": "913e102f-e4d7-4ac8-9b76-3a0c1b3790b9"
   },
   "source": [
    "#### Extract information and format it as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efdfe0f7-678c-4ad6-bdcb-4cc299512570",
   "metadata": {
    "id": "efdfe0f7-678c-4ad6-bdcb-4cc299512570"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\" A bright and promising wildlife biologist \\\n",
    "named Jesse Plank (Amara Patel) is determined to make her \\\n",
    "mark on the world.\n",
    "Jesse moves to Texas for what she believes is her dream job,\n",
    "only to discover a dark secret that will make \\\n",
    "her question everything.\n",
    "In the new lab she quickly befriends the outgoing \\\n",
    "lab tech named Maya Jones (Chloe Nguyen),\n",
    "and the lab director Sam Porter (Fredrik Johansson).\n",
    "Together the trio work long hours on their research \\\n",
    "in a hope to change the world for good.\n",
    "Along the way they meet the comical \\\n",
    "Brenna Ode (Eleanor Garcia) who is a marketing lead \\\n",
    "at the research institute,\n",
    "and marine biologist Siri Teller (Freya Johansson).\n",
    "\n",
    "Extract the characters, their jobs \\\n",
    "and the actors who played them from the above message as a table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92efacae-afe4-4cd2-9d6d-cb565d102ccc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92efacae-afe4-4cd2-9d6d-cb565d102ccc",
    "outputId": "d7bfac89-66a0-4a9f-da07-62c67f6a6557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Character | Job | Actor |\n",
      "|---|---|---|\n",
      "| Jesse Plank | Wildlife Biologist | Amara Patel |\n",
      "| Maya Jones | Lab Tech | Chloe Nguyen |\n",
      "| Sam Porter | Lab Director | Fredrik Johansson |\n",
      "| Brenna Ode | Marketing Lead | Eleanor Garcia |\n",
      "| Siri Teller | Marine Biologist | Freya Johansson |\n"
     ]
    }
   ],
   "source": [
    "response = generation_model.predict(prompt=prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80be8ca-4732-4b39-a5e6-5db1d3b02218",
   "metadata": {
    "id": "d80be8ca-4732-4b39-a5e6-5db1d3b02218"
   },
   "source": [
    "- You can copy-paste the text into a markdown cell to see if it displays a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18816c8e-a473-4eb5-8983-7c5f80f8aa15",
   "metadata": {
    "id": "18816c8e-a473-4eb5-8983-7c5f80f8aa15"
   },
   "source": [
    "| Character | Job | Actor |\n",
    "|---|---|---|\n",
    "| Jesse Plank | Wildlife Biologist | Amara Patel |\n",
    "| Maya Jones | Lab Tech | Chloe Nguyen |\n",
    "| Sam Porter | Lab Director | Fredrik Johansson |\n",
    "| Brenna Ode | Marketing Lead | Eleanor Garcia |\n",
    "| Siri Teller | Marine Biologist | Freya Johansson |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y7u9aBd-iSq1",
   "metadata": {
    "id": "Y7u9aBd-iSq1"
   },
   "source": [
    "## Indeed..\n",
    "\n",
    "| Character | Job | Actor |\n",
    "|---|---|---|\n",
    "| Jesse Plank | Wildlife Biologist | Amara Patel |\n",
    "| Maya Jones | Lab Tech | Chloe Nguyen |\n",
    "| Sam Porter | Lab Director | Fredrik Johansson |\n",
    "| Brenna Ode | Marketing Lead | Eleanor Garcia |\n",
    "| Siri Teller | Marine Biologist | Freya Johansson |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9c406-2b3f-4fb0-a181-201091013864",
   "metadata": {
    "id": "87b9c406-2b3f-4fb0-a181-201091013864"
   },
   "source": [
    "### Adjusting Creativity/Randomness\n",
    "- You can control the behavior of the language model's decoding strategy by adjusting the temperature, top-k, and top-n parameters.\n",
    "- For tasks for which you want the model to consistently output the same result for the same input, (such as classification or information extraction), set temperature to zero.\n",
    "- For tasks where you desire more creativity, such as brainstorming, summarization, choose a higher temperature (up to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f629e383-b4ad-4fa6-a949-435dd1f04da6",
   "metadata": {
    "id": "f629e383-b4ad-4fa6-a949-435dd1f04da6"
   },
   "outputs": [],
   "source": [
    "temperature = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84519df4-9b6f-471c-8183-27cc4a151d7d",
   "metadata": {
    "id": "84519df4-9b6f-471c-8183-27cc4a151d7d"
   },
   "outputs": [],
   "source": [
    "prompt = \"Complete the sentence: \\\n",
    "As I prepared the picture frame, \\\n",
    "I reached into my toolkit to fetch my:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a2fe53e-d885-411c-bb2e-c3940884d30f",
   "metadata": {
    "id": "2a2fe53e-d885-411c-bb2e-c3940884d30f"
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2be86a8a-a58f-4b9e-bd66-9eced64e549c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2be86a8a-a58f-4b9e-bd66-9eced64e549c",
    "outputId": "9b078960-a429-4401-bd32-6b83b96b651d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 0.0]\n",
      "As I prepared the picture frame, I reached into my toolkit to fetch my hammer.\n"
     ]
    }
   ],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "zf8NfOaUjEm_",
   "metadata": {
    "id": "zf8NfOaUjEm_"
   },
   "outputs": [],
   "source": [
    "prompt = \"The garden was full of beautiful:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "RqTPXTdmjKGU",
   "metadata": {
    "id": "RqTPXTdmjKGU"
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cx8hY5bLjNTg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cx8hY5bLjNTg",
    "outputId": "df85156d-917f-410c-d775-e275675e2dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 1.0]\n",
      "The garden was full of beautiful roses, lilies, and daffodils.\n"
     ]
    }
   ],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gzahU_TvjQzt",
   "metadata": {
    "id": "gzahU_TvjQzt"
   },
   "source": [
    "## what's happening here is there are probabilities over all these tokens: flowers, trees, herbs, .... bugs... and they all have a probabiliti, like 0.5, 0.23, 0.22, 0.03.... Which one to choose?\n",
    "## decoding strategies:\n",
    "### 1. Greedy decoding; the one with the highest probability. It may generate some interesting or repetitive answer.\n",
    "### 2. Random sample: Use the probabilities to sample a random token. may end up with some unsual tokens, or some unusual responses. We can control how random it would be though.   One way to control the randomness, it is Temperature.       lower temperartue, it would generate the most likely ones (predictable). If you want something that is less likely, then you'd better choose a higher temperature.\n",
    "\n",
    "#### softmax: normal softmax   with temperature would become softmax that have a theta (temperature value) at the dinominator under the Z_i (input parameter).\n",
    "\n",
    "### logit values --> get\n",
    "\n",
    "\n",
    "#### Temperature of 0: deterministic choice: not creative at all but very reliabe.\n",
    "\n",
    "### increasing the temperature, it means it would be creative but not very reliable. (completely random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U5nRrrl9kbdw",
   "metadata": {
    "id": "U5nRrrl9kbdw"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vnUibG51jQU5",
   "metadata": {
    "id": "vnUibG51jQU5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac5bd8e5-8fdc-4d94-acf9-10b7d3b34ba8",
   "metadata": {
    "id": "ac5bd8e5-8fdc-4d94-acf9-10b7d3b34ba8"
   },
   "outputs": [],
   "source": [
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0bf346d-2617-460e-bc47-f331b92ed053",
   "metadata": {
    "id": "f0bf346d-2617-460e-bc47-f331b92ed053"
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "67e25ae3-a1ae-4379-ad32-8d79e52b7590",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67e25ae3-a1ae-4379-ad32-8d79e52b7590",
    "outputId": "e9de7753-3de0-4f54-9201-ab21cbe18eef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 1]\n",
      "The garden was full of beautiful flowers and roses.\n",
      "\n",
      "The garden is full of beautiful things. We can say: The garden is full of beautiful flowers and roses.\n"
     ]
    }
   ],
   "source": [
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "LV1U6q6vku0J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LV1U6q6vku0J",
    "outputId": "460d75b9-b47e-46c7-ef00-55b8e97b4c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 0]\n",
      "The garden was full of beautiful flowers.\n"
     ]
    }
   ],
   "source": [
    "temperature = 0\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")\n",
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "CPSkqqn-k2Ft",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPSkqqn-k2Ft",
    "outputId": "f3af089e-970e-4f0d-c50c-57f0b72a97f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature = 0.5]\n",
      "The garden was full of beautiful flowers.\n",
      "\n",
      "\"Full of\" means \"containing a lot of\". The garden was full of beautiful things. We can say \"The garden was full of beautiful flowers\".\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=temperature,\n",
    ")\n",
    "print(f\"[temperature = {temperature}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cEJgB78PlJJd",
   "metadata": {
    "id": "cEJgB78PlJJd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fea806c4-f955-4434-89a5-57fd9083e9d3",
   "metadata": {
    "id": "fea806c4-f955-4434-89a5-57fd9083e9d3"
   },
   "source": [
    "#### Top P\n",
    "- Top p: sample the minimum set of tokens whose probabilities add up to probability `p` or greater.\n",
    "- The default value for `top_p` is `0.95`.\n",
    "- If you want to adjust `top_p` and `top_k` and see different results, remember to set `temperature` to be greater than zero, otherwise the model will always choose the token with the highest probability.\n",
    "\n",
    "\n",
    "#### top_p: accumulative probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af98a491-6da8-4cca-bf16-85f9547e7fe5",
   "metadata": {
    "id": "af98a491-6da8-4cca-bf16-85f9547e7fe5"
   },
   "outputs": [],
   "source": [
    "top_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e195211-60ea-4103-b083-19e78a26c920",
   "metadata": {
    "id": "2e195211-60ea-4103-b083-19e78a26c920"
   },
   "outputs": [],
   "source": [
    "prompt = \"Write an advertisement for jackets \\\n",
    "that involves blue elephants and avocados.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28093674-1a83-4abb-999a-030df75b2125",
   "metadata": {
    "id": "28093674-1a83-4abb-999a-030df75b2125"
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=0.9,\n",
    "    top_p=top_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30a4ce9d-7520-44bc-af24-dc1b6ad61b50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30a4ce9d-7520-44bc-af24-dc1b6ad61b50",
    "outputId": "213ecbc4-8d7b-4bb8-a0a5-044c6ee63838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 0.7]\n",
      "**Introducing the new Blue Elephant Avocado Jacket!**\n",
      "\n",
      "This jacket is the perfect way to show off your love of both blue elephants and avocados. It's made of a soft, lightweight fabric that's perfect for any season. And the bright colors will make you stand out from the crowd.\n",
      "\n",
      "The Blue Elephant Avocado Jacket is perfect for any occasion. Wear it to a concert, a party, or even just a casual day out. You're sure to turn heads wherever you go.\n",
      "\n",
      "So what are you waiting for? Order your Blue Elephant Avocado Jacket today!\n",
      "\n",
      "**Here are some of the\n"
     ]
    }
   ],
   "source": [
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02817243-ef3f-440a-8846-33dc46963b41",
   "metadata": {
    "id": "02817243-ef3f-440a-8846-33dc46963b41"
   },
   "source": [
    "#### Top k\n",
    "- The default value for `top_k` is `40`.\n",
    "- You can set `top_k` to values between `1` and `40`.\n",
    "- The decoding strategy applies `top_k`, then `top_p`, then `temperature` (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6603255-1e19-47ce-8a77-634b33c1e99f",
   "metadata": {
    "id": "f6603255-1e19-47ce-8a77-634b33c1e99f"
   },
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "top_p = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef545757-b37c-4b39-b7e9-3ebc4ec1d910",
   "metadata": {
    "id": "ef545757-b37c-4b39-b7e9-3ebc4ec1d910"
   },
   "outputs": [],
   "source": [
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=0.9,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d09c4f8d-df9d-4d61-9ab6-19958f8b4129",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d09c4f8d-df9d-4d61-9ab6-19958f8b4129",
    "outputId": "87dcda1b-4b77-4e1a-f98d-aa6b4c46c229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 0.7]\n",
      "**Introducing the new Blue Elephant Avocado Jacket!**\n",
      "\n",
      "This jacket is made from soft, durable fabric that will keep you warm and comfortable all winter long. It features a classic button-down design with a front pocket and a hood. The blue color is perfect for any season, and the elephant and avocado print is sure to turn heads.\n",
      "\n",
      "Whether you're running errands, going to school, or just hanging out, the Blue Elephant Avocado Jacket is the perfect way to stay stylish and warm. Order yours today!\n",
      "\n",
      "**Here are some of the benefits of owning a Blue Elephant Avocado Jacket:**\n",
      "\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "lnq0N_Q7l3-0",
   "metadata": {
    "id": "lnq0N_Q7l3-0"
   },
   "outputs": [],
   "source": [
    "prompt = \"Compare Mac M3 and Studio M2? I am a graduate student in cs, \\\n",
    "which one should I get? only focus on performance. Respond in bullet points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "005c5ff2-03f0-4dcd-a808-c92c9c5bbb0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "005c5ff2-03f0-4dcd-a808-c92c9c5bbb0a",
    "outputId": "89fcebe1-4dac-4376-a049-2cd0d3b0b589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[top_p = 0.7]\n",
      "**Mac M3**\n",
      "\n",
      "* More portable and lightweight\n",
      "* Better battery life\n",
      "* Cheaper\n",
      "\n",
      "**Mac Studio M2**\n",
      "\n",
      "* More powerful\n",
      "* More ports\n",
      "* Better cooling\n",
      "\n",
      "Overall, the Mac Studio M2 is the better choice for graduate students in CS who need a powerful and performant machine. However, if you are looking for a more portable and affordable option, the Mac M3 is a good choice.\n"
     ]
    }
   ],
   "source": [
    "top_k = 20\n",
    "top_p = 0.7\n",
    "\n",
    "response = generation_model.predict(\n",
    "    prompt=prompt,\n",
    "    temperature=0.9,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    ")\n",
    "\n",
    "print(f\"[top_p = {top_p}]\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WTbdtDGPl0lv",
   "metadata": {
    "id": "WTbdtDGPl0lv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
