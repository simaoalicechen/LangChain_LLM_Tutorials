{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2487e8e",
   "metadata": {},
   "source": [
    "## Llama 2 7B\n",
    "- 28 GB storage in FP32 (32-bit precision)\n",
    "- Reduce to ~4 GB if store in 4-bit precision, in \"GGUF\" format\n",
    "- can run locally?!\n",
    "\n",
    "\n",
    "##### Huggingface has llama-2-7B GGUF model \n",
    "- perfomance degradation? check out Hugging Face open LLM leaderboard. (https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard), (https://huggingface.co/open-llm-leaderboard)\n",
    "\n",
    "\n",
    "#### Fine Tuning a quantized model\n",
    "\n",
    "- benefits of fine-tuning a quantized model:\n",
    "    - recover the accuracy from quantization (a)\n",
    "    - tailor your model for specific use-cases and applications (b)\n",
    "    \n",
    "\n",
    "#### fine tune with quantization aware training (QAT)\n",
    "\n",
    "- Fine-tune the model in a way that its quantized version will perform optimally. \n",
    "    - Not compatible with Post Traning Quantization (PTQ) techniques. \n",
    "    - The linear quantization method you learned in this course is an example of Post Traning Quantization. \n",
    "    \n",
    "    \n",
    "#### Parmeters efficient fine-tuning (PEFT)\n",
    "- Significantly reduce the number of trainable parameters of a model while keeping the same performance as full fine-tuning \n",
    "    - PEFT + QLoRA\n",
    "    - https://pytorch.org/blog/finetune-llms (good)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238cc89",
   "metadata": {},
   "source": [
    "### LLM int8 paper:\n",
    "\n",
    "- ####  1. Vector-Wise Quantization\n",
    "\n",
    "    - When transformers do matrix multiplication -- a key operation in deep learning models, we want to reduce the precision of the numbers (such as from 16-bit or 32-bit to 8-bit covered above) to save memory and computation power. However, if we just apply 8-bit quantization across the whole matrix, it can reduce the precision of some important numbers, leading to performance loss.\n",
    "\n",
    "    - Key Idea: Instead of applying one scaling factor to the entire matrix, vector-wise quantization applies a separate scaling factor to each row and column of the matrix. This way, each row and column can maintain the precision of its own values without being overly influenced by the largest values in the matrix.\n",
    "\n",
    "    - How it works in reality: Let’s say you have a matrix \n",
    "    X of size  s×h (where s is the sequence length and h is the hidden dimension).\n",
    "    Each row of this matrix represents a sequence of tokens, and each column represents some feature.\n",
    "    Instead of scaling the entire matrix with one number, vector-wise quantization scales each row of \n",
    "    X using a unique normalization constant C_x, and each column of \n",
    "    W (the weight matrix) with another constant c_w.   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df25ca",
   "metadata": {},
   "source": [
    "- The formula for quantization is: $X_{\\text{int8}} = \\left\\lfloor \\frac{\\max\\left(\\left| X_i \\right|\\right)}{127} \\cdot X_i \\right\\rfloor$ This means you scale each element of X_i to fit into the 8-bit range [−127,127], using the max value from that row (or column).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be65b39",
   "metadata": {},
   "source": [
    "- So.. the maxtrix, can be represented as: (if we have to put this in math form)\n",
    "\n",
    "    - $\\hat{C} = \\frac{1}{C_x \\otimes C_w} \\cdot \\hat{X} \\cdot \\hat{W}$\n",
    "\n",
    "    where: \n",
    "\n",
    "    C_hat is the quantized result.\n",
    "    C_xand C_w are the row and column scaling factors.\n",
    "    X_hat and W_hat are the quantized versions of the original matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c9317",
   "metadata": {},
   "source": [
    "- ####  2. Mixed-Precision Decomposition \n",
    "\n",
    "    - As transformers get even larger (like above 6.7 billion parameters), certain \"outliers\" start to appear in the matrix. These outliers are large, systematic values that significantly affect performance if quantized to 8-bit. If we quantize these large outliers along with the rest of the matrix, they would cause severe performance degradation.\n",
    "\n",
    "    - Introducing mixed-precision decomposition, which separates these outliers and processes them in higher precision (16-bit), while keeping the rest of the matrix in 8-bit. This way, most of the matrix can stay in memory-efficient 8-bit, but the outliers are handled with greater precision to avoid performance loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424e913",
   "metadata": {},
   "source": [
    "- #### How it works in reality: \n",
    "\n",
    "    1) Identify Outliers: In the matrix multiplication, outliers are dimensions (or features) that have very large values (about 20 times larger than other values). These outliers start to appear in about 25% of transformer layers and affect around 0.1% of the features.\n",
    "\n",
    "    2) Separate Outliers: The matrix is decomposed into two parts: one containing the outliers, and the other containing the regular values. Outliers are processed with 16-bit precision, while regular values are processed with 8-bit precision.\n",
    "\n",
    "    3) Perform Mixed-Precision Matrix Multiplication: For the outliers: the matrix multiplication is done using 16-bit floating point (FP16) precision. For the regular values: the matrix multiplication is done using 8-bit precision, using vector-wise quantizatio (there is a formular to nicely summarize this. see paper)\n",
    "    \n",
    "    4) Recombine the Results: After the matrix multiplication, the outputs from the 8-bit and 16-bit operations are combined to give the final result.\n",
    "\n",
    "    5) Why It Works?:\n",
    "    By isolating the outliers and processing them in 16-bit, the method avoids the performance degradation caused by quantizing these critical values to 8-bit. Meanwhile, more than 99.9% of the matrix is still processed in 8-bit, maintaining the memory and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1421091",
   "metadata": {},
   "source": [
    "in conclution: vector-wise quantization improves the precision of matrix multiplication by applying different scaling factors for rows and columns. Mixed-precision decomposition handles large, systematic outliers in higher precision (16-bit) while keeping the rest of the matrix in 8-bit, ensuring that performance is maintained even for large transformer models. This combined approach allows transformer models with up to 175 billion parameters to run with reduced memory requirements and no loss in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6acb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
