{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNNfYAwJEDL2"
   },
   "source": [
    "You can download the `requirements.txt` for this course from the workspace of this lab. `File --> Open...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8teiBZ6dP5g"
   },
   "source": [
    "# Lesson 2: Data Types and Sizes\n",
    "\n",
    "In this lab, you will learn about the common data types used to store the parameters of machine learning models.\n",
    "\n",
    "\n",
    "The libraries are already installed in the classroom.  If you're running this notebook on your own machine, you can install the following:\n",
    "\n",
    "```Python\n",
    "!pip install torch==2.1.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8a29116"
   },
   "source": [
    "# Model Compression -Pruning\n",
    "\n",
    "1. Remove connections that do not improve the model\n",
    "\n",
    "Before pruning:\n",
    "\n",
    "pruning synapses -->\n",
    "\n",
    "pruning neurons -->\n",
    "\n",
    "After pruning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d668c4fd"
   },
   "source": [
    "# Model compression - knowledge Distillation\n",
    "\n",
    "2. Train a smaller model (student) using the original model (instructor).\n",
    "\n",
    "origianl model (instructor)\n",
    "\n",
    "-->\n",
    "\n",
    "smaller model (student)\n",
    "\n",
    "\n",
    "transfer knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31df1ed9"
   },
   "source": [
    "In a nueral network, you can quantize the weights: w\n",
    "Neural networks aprameters\n",
    "\n",
    "The activations: a\n",
    "    Values that propagate through teh layers of the neural network\n",
    "    \n",
    "For example, in a linear layer\n",
    "\n",
    "a = g(w*x+b)\n",
    "a is the activation\n",
    "w and b are the weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7213bf51"
   },
   "source": [
    "# Model compression - Quantization\n",
    "\n",
    "-idea: Store the parameters of the model in lower precision (meaning less bytes, less numbers behind decimals).\n",
    "\n",
    "An example: a weight matrix may carry:\n",
    "\n",
    "FP32: 4 bytes to store each other Total: 36 bytes.\n",
    "\n",
    "==> INT8: 1 byte to store each value: total 9 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62230fb2"
   },
   "source": [
    "# What is covered in this course:\n",
    "\n",
    "Data representation in ML Dtypes\n",
    "\n",
    "- integer (int8)\n",
    "\n",
    "- Floating Point (FP32, F16, BF16)\n",
    "\n",
    "-- Sign: 1 bit\n",
    "-- Exponent (range): 8 bit\n",
    "-- Fraction (precision): 23 bit\n",
    "-- total: 32 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mEqVNJoIFDS"
   },
   "source": [
    "# Integer\n",
    "\n",
    "- unsigned integer:\n",
    "  - range for n-bits: [0, 2^n - 1]\n",
    "  Example with 8-bit (torch.unit8): [0, 255]\n",
    "\n",
    "- Signed integer two's complement representation\n",
    "  - range for n- bits: [-2^{n-1}, 2^{n-1} -1]\n",
    "  Example with 8-bit (torch.int8): [-128, 127]\n",
    "\n",
    "Signed: negative to positive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvCdhjAJL-Q-"
   },
   "source": [
    "# Integer\n",
    "\n",
    "Signed integer 2's complement representation\n",
    "  - range for n-bits: [-2^{n-1}, 2^{n-1} - 1]\n",
    "  Example with 4-bit : Addition\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AE8B-s3EMSUP"
   },
   "source": [
    "## in torch, sign integers is very easy, just need to use the correct syntex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbM6rqy_L917"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2n3AbcSIEkY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SEbvWrMHJny"
   },
   "source": [
    "# Flaoting Point\n",
    "\n",
    "3 components in flaoting point:\n",
    "- sign: positive/negative (always 1 bit)\n",
    "- exponent (range): impact the representable range of the number\n",
    "- Fraction (precision): impact on the precision of the number\n",
    "\n",
    "FP32, BF16, FP16, FP8 are floating point format with a specific number of bits for exponent and the fraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uol2ZmlKHqb3"
   },
   "source": [
    "FP 32:\n",
    "\n",
    "- sign 1 bit\n",
    "- exponeht (range): 8bit\n",
    "- fraction (precision): 23 bit\n",
    "- total: 32 bit\n",
    "\n",
    "Subnormal values (E = 0)\n",
    "(-1)^SF2^{-126}\n",
    "\n",
    "Normal values: (E != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUGPCWHhOPdI"
   },
   "source": [
    "# Comparison\n",
    "\n",
    "Data Type: FP32, FP16, BF16\n",
    "Precision: best, better, good\n",
    "Maximum: ~10^{38}, ~10^{04}, ~10^{38}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMvC_sTiRscK"
   },
   "source": [
    "# Floating Point - PyTorch Downcasting\n",
    "\n",
    "## When a higher data type, such as a float is converted to a lower data type, such as an integer.\n",
    "## It usually results in a loss of data.\n",
    "## There is impact on matric multiplication.\n",
    "\n",
    "## the value will be converted to the lowest and closest value of the lower datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjC3toN0Sy89"
   },
   "source": [
    "Flaoting Point -- PyTorch Downcasting\n",
    "\n",
    "Advantages:\n",
    "1. Reduced memory footprint\n",
    "- more efficient use of GPU memory\n",
    "- Enables the training of larger models\n",
    "- Enavles larger batch sizes\n",
    "\n",
    "2. Increased compute and speed\n",
    "- Computation using low precision (fp16, bf16) can be faster than fp32 since it requries less memory.\n",
    "\n",
    "- Depends on the hardware (e.g. Google TPU, NVIDIA A100...)\n",
    "\n",
    "Disadvantages:\n",
    "- Less precise: We are using less memory, hence the computaion is less precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzv4nGcLTq31"
   },
   "source": [
    "# Floating Point - PyTorch Downcasting\n",
    "\n",
    "Use case:\n",
    "- Mixed precision training\n",
    "  - Do computation in smaller precision (FP15, BF16, FP8)\n",
    "  - Store and update the weights in higher precision (FP32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIYNIPz0SOc4"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hxr7qReOO5X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySRvbd7eN1B-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ks3h9JXH9fr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQEg5riYH0dH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3109501c"
   },
   "source": [
    "# Quantization\n",
    "\n",
    "- Linear quantization explained\n",
    "\n",
    "FP32 to INT8\n",
    "\n",
    "- Apply linear quantization to real models using Quanto (a hf model)\n",
    "\n",
    "- quantization applications on large language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 30,
    "id": "nFC9wgzxBZvz"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW7psx41rn0h"
   },
   "source": [
    "### Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "R0j4Ua5yEDL8",
    "outputId": "b22fd182-2ee7-4031-a721-d0b656fc617f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=0, max=255, dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of `8-bit unsigned integer`\n",
    "torch.iinfo(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "KxIuiot9EDL8",
    "outputId": "be287aa9-0cda-4028-fa5a-1074320e1999"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-128, max=127, dtype=int8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of `8-bit (signed) integer`\n",
    "torch.iinfo(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "K7lWHlrYEDL9",
    "outputId": "6721d9e8-9e6c-403b-eea6-b4a23627cb00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-9.22337e+18, max=9.22337e+18, dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `64-bit (signed) integer`\n",
    "torch.iinfo(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "Y1z0Oio-EDL9",
    "outputId": "97d71f36-27f5-4bbf-8bb0-882aac735349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `32-bit (signed) integer`\n",
    "torch.iinfo(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "gT_PxqwMEDL9",
    "outputId": "8d0dca24-f321-4ac4-f7b3-7f8dda99bb5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-32768, max=32767, dtype=int16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `16-bit (signed) integer`\n",
    "torch.iinfo(torch.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9b0sfqIEDL9"
   },
   "source": [
    "### Floating Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 62,
    "id": "qNT4EbQdEDL9"
   },
   "outputs": [],
   "source": [
    "# by default, python stores float data in fp64\n",
    "value = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "height": 30,
    "id": "3ukD6u5gEDL9",
    "outputId": "689438e0-0908-489f-b034-378a6e8055a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.333333333333333314829616256247390992939472198486328125000000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(value, '.60f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 62,
    "id": "6dCxzrI3EDL-"
   },
   "outputs": [],
   "source": [
    "# 64-bit floating point\n",
    "tensor_fp64 = torch.tensor(value, dtype = torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 45,
    "id": "QCC7qJ-1EDL-",
    "outputId": "8fe5d775-54e4-4945-b2b2-7119f4ed9559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 79,
    "id": "0TLyzd2hEDL-"
   },
   "outputs": [],
   "source": [
    "tensor_fp32 = torch.tensor(value, dtype = torch.float32)\n",
    "tensor_fp16 = torch.tensor(value, dtype = torch.float16)\n",
    "tensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 96,
    "id": "8DmxeeEPEDL-",
    "outputId": "75b56ee9-27d5-4c06-f3d3-40028ef6e5da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n",
      "fp32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\n",
      "fp16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\n",
      "bf16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\n",
    "print(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\n",
    "print(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\n",
    "print(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 62,
    "id": "hUukczHrBodt",
    "outputId": "c0e11528-d94e-4d73-a6e6-9c78818ebbc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of `16-bit brain floating point`\n",
    "torch.finfo(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "0nU4LscCEDL-",
    "outputId": "dbcb2bd4-8ac5-4e61-da30-87f2131c8116"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Information of `32-bit floating point`\n",
    "torch.finfo(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "sZyGuL9AEDL_",
    "outputId": "9da6bdd3-4c7f-45b9-d68f-ae8367e2b467"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `16-bit floating point`\n",
    "torch.finfo(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "-WEnUrPtEDL_",
    "outputId": "1d2c0fe6-a524-4583-d867-10a2c17878b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1e-15, min=-1.79769e+308, max=1.79769e+308, eps=2.22045e-16, smallest_normal=2.22507e-308, tiny=2.22507e-308, dtype=float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Information of `64-bit floating point`\n",
    "torch.finfo(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DbtWVVc_jiW"
   },
   "source": [
    "### Downcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 62,
    "id": "JHDsqfauBID9"
   },
   "outputs": [],
   "source": [
    "# random pytorch tensor: float32, size=1000\n",
    "tensor_fp32 = torch.rand(1000, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkyqjvM1EDL_"
   },
   "source": [
    "**Note:** As it is random, the values you get will be different from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "Vup4ffhrEDL_",
    "outputId": "6e2c7b23-80e9-4438-dc16-ec30f4911943"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0507, 0.7246, 0.1621, 0.9497, 0.4454])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 elements of the random tensor\n",
    "tensor_fp32[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 62,
    "id": "xXLANbSx_nx4"
   },
   "outputs": [],
   "source": [
    "# downcast the tensor to bfloat16 using the \"to\" method\n",
    "tensor_fp32_to_bf16 = tensor_fp32.to(dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "VZuFqEucEDL_",
    "outputId": "58bec743-6a28-417b-bb06-b8f90845d747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0508, 0.7227, 0.1621, 0.9492, 0.4453], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_fp32_to_bf16[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 62,
    "id": "iF_VUGc7EDMA"
   },
   "outputs": [],
   "source": [
    "# tensor_fp32 x tensor_fp32\n",
    "m_float32 = torch.dot(tensor_fp32, tensor_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "iJf4KeAiEDMA",
    "outputId": "5d657075-557e-4c64-d1db-03e288037bd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(342.1369)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "height": 62,
    "id": "E3qpc82REDMA"
   },
   "outputs": [],
   "source": [
    "# tensor_fp32_to_bf16 x tensor_fp32_to_bf16\n",
    "m_bfloat16 = torch.dot(tensor_fp32_to_bf16, tensor_fp32_to_bf16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "yJRAmzDREDMA",
    "outputId": "257ec8d2-d940-40c3-96c9-b69f1a290be0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(342., dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "braPqZ01EDMA"
   },
   "source": [
    "#### Note\n",
    "- You'll use \"downcasting\" as a simple form of quantization in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30,
    "id": "XI1E79cWEDMA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "x_9J8WavkQGl",
    "sulufN1wkK_L",
    "hqPUM0f8oGgf",
    "TgmPMm-ZvdXX",
    "MJuh8aqo9LO-",
    "3bOevU0Ez4KB",
    "LgtYIhSf0Uu0",
    "9DH7-tDDvK5N",
    "TuZhoPK6weuR",
    "WAOeSNraxZeF",
    "UGXo-IljxmNG",
    "_N3G-dX32awT",
    "hW7psx41rn0h",
    "OmcNPw6zlRp7",
    "3_zvORGrnTR8",
    "jh9IuiovrnoF",
    "isDcbkXxxiTf",
    "wM-xkASw1odi",
    "3DbtWVVc_jiW",
    "ViImpV5rAvyp",
    "RdcyknnjBD99",
    "-eYj4UUXCAlJ",
    "MCLe1N4GCSQT",
    "GIY7IrOv_3cD",
    "8HlFKDBKGNG8"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
