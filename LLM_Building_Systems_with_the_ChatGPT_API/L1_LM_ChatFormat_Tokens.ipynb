{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
   "metadata": {
    "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630"
   },
   "source": [
    "# L1 Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
   "metadata": {
    "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f"
   },
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cF70g9dyAxbI",
   "metadata": {
    "id": "cF70g9dyAxbI"
   },
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install openai\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19cd4e96",
   "metadata": {
    "height": 132,
    "id": "19cd4e96"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "openai_api_key = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
   "metadata": {
    "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c"
   },
   "source": [
    "#### helper function\n",
    "This may look familiar if you took the earlier course \"ChatGPT Prompt Engineering for Developers\" Course.\n",
    "\n",
    "Throughout this course, we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat).\n",
    "\n",
    "This helper function will make it easier to use prompts and look at the generated outputs.\n",
    "\n",
    "**Note**: In June 2023, OpenAI updated gpt-3.5-turbo. The results you see in the notebook may be slightly different than those in the video. Some of the prompts have also been slightly modified to produce the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed96988",
   "metadata": {
    "height": 149,
    "id": "1ed96988"
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73207e48-02a1-4e7e-bf41-ba29ee11ecfc",
   "metadata": {
    "id": "73207e48-02a1-4e7e-bf41-ba29ee11ecfc"
   },
   "source": [
    "**Note**: This and all other lab notebooks of this course use OpenAI library version `0.27.0`.\n",
    "\n",
    "In order to use the OpenAI library version `1.0.0`, here is the code that you would use instead for the get_completion function:\n",
    "\n",
    "```python\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "D3wsAJPZBJgj",
   "metadata": {
    "id": "D3wsAJPZBJgj"
   },
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key = openai_api_key)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10a390-2461-447d-bf8b-8498db404c44",
   "metadata": {
    "id": "fe10a390-2461-447d-bf8b-8498db404c44"
   },
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1cc57b2",
   "metadata": {
    "height": 30,
    "id": "e1cc57b2"
   },
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76774108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "76774108",
    "outputId": "4a2ad7b4-9db4-4d85-a1c6-7638125b7a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
   "metadata": {
    "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63"
   },
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2d9e40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 64,
    "id": "cc2d9e40",
    "outputId": "e5a78e79-a0e6-4986-a6a6-02f432e4600d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilpolol\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop \\\n",
    "and reverse them\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# This way, it will only reverse the tokens in the word, it will not reverse the letters.\n",
    "# This is because, llm do not treat on letter level, it treats on token lever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
   "metadata": {
    "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f"
   },
   "source": [
    "\"lollipop\" in reverse should be \"popillol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37cab84f",
   "metadata": {
    "height": 47,
    "id": "37cab84f"
   },
   "outputs": [],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rH2tr2vBCHyw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rH2tr2vBCHyw",
    "outputId": "41a8a40d-751c-4cd2-8808-25eab82329c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a language model AI created to assist with answering questions and providing information to the best of my abilities. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"who are you?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ZNYWcA1LC6Tv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNYWcA1LC6Tv",
    "outputId": "59c3ee12-ae6d-4f42-e10f-ad7c9b79eb7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starbucks is a multinational chain of coffeehouses and roastery reserves headquartered in Seattle, Washington. The company was founded in 1971 and has since grown to become one of the largest and most recognizable coffee brands in the world. Starbucks is known for its wide variety of coffee drinks, pastries, and other food items, as well as its commitment to ethical sourcing and sustainability practices.\n",
      "\n",
      "Source: https://www.starbucks.com/about-us/\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Tell me something about Starbucks, and tell me where do you get source from?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1577c561",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "height": 30,
    "id": "1577c561",
    "outputId": "01887902-81b8-49ef-9795-12dd556341eb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'p-o-p-i-l-l-o-l'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
   "metadata": {
    "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd"
   },
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f89efad",
   "metadata": {
    "height": 200,
    "id": "8f89efad"
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages,\n",
    "                                 model=\"gpt-3.5-turbo\",\n",
    "                                 temperature=0,\n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pDHNz50DGgTM",
   "metadata": {
    "id": "pDHNz50DGgTM"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key= openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b28c3424",
   "metadata": {
    "height": 183,
    "id": "b28c3424"
   },
   "outputs": [],
   "source": [
    "messages =  [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "-Ex4VjixGXp-",
   "metadata": {
    "id": "-Ex4VjixGXp-"
   },
   "outputs": [],
   "source": [
    "# New Syntex\n",
    "\n",
    "response = client.completions.create(\n",
    "  model=\"gpt-3.5-turbo-instruct\",\n",
    "  prompt = messages,\n",
    "  # messages=messages,\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56c6978d",
   "metadata": {
    "height": 183,
    "id": "56c6978d"
   },
   "outputs": [],
   "source": [
    "# Old Syntex\n",
    "\n",
    "# # length\n",
    "# messages =  [\n",
    "# {'role':'system',\n",
    "#  'content':'All your responses must be \\\n",
    "# one sentence long.'},\n",
    "# {'role':'user',\n",
    "#  'content':'write me a story about a happy carrot'},\n",
    "# ]\n",
    "# response = get_completion_from_messages(messages, temperature =1)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14fd6331",
   "metadata": {
    "height": 217,
    "id": "14fd6331"
   },
   "outputs": [],
   "source": [
    "# # combined\n",
    "# messages =  [\n",
    "# {'role':'system',\n",
    "#  'content':\"\"\"You are an assistant who \\\n",
    "# responds in the style of Dr Seuss. \\\n",
    "# All your responses must be one sentence long.\"\"\"},\n",
    "# {'role':'user',\n",
    "#  'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "# ]\n",
    "# response = get_completion_from_messages(messages,\n",
    "#                                         temperature =1)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89a70c79",
   "metadata": {
    "height": 370,
    "id": "89a70c79"
   },
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages,\n",
    "                                   model=\"gpt-3.5-turbo\",\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=500):\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message[\"content\"]\n",
    "\n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a64cf3c6",
   "metadata": {
    "height": 166,
    "id": "a64cf3c6"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\\n",
    " about a happy carrot\"\"\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "H8AQoYFxJRlu",
   "metadata": {
    "id": "H8AQoYFxJRlu"
   },
   "outputs": [],
   "source": [
    "# old syntex\n",
    "# response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd8fbd4",
   "metadata": {
    "height": 30,
    "id": "cfd8fbd4"
   },
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "352ad320",
   "metadata": {
    "height": 30,
    "id": "352ad320"
   },
   "outputs": [],
   "source": [
    "# print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65372cdd-d869-4768-947a-0173e7f96335",
   "metadata": {
    "id": "65372cdd-d869-4768-947a-0173e7f96335"
   },
   "source": [
    "#### Notes on using the OpenAI API outside of this classroom\n",
    "\n",
    "To install the OpenAI Python library:\n",
    "```\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    " ```\n",
    " !export OPENAI_API_KEY='sk-...'\n",
    " ```\n",
    "\n",
    "Or, set `openai.api_key` to its value:\n",
    "\n",
    "```\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f889c1-f2e4-40a5-bd27-164facb54402",
   "metadata": {
    "id": "d8f889c1-f2e4-40a5-bd27-164facb54402"
   },
   "source": [
    "#### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294dcc54-f941-422e-8602-d8e78a0da093",
   "metadata": {
    "height": 30,
    "id": "294dcc54-f941-422e-8602-d8e78a0da093"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
