{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoQvWCNh37yB"
   },
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWVqI7UD6BEj"
   },
   "source": [
    "# What is LangChain and memorry\n",
    "\n",
    "### Use the memory as a conversation buffer memory\n",
    "### conversation.predict(input = \"Hi, my name is Andrew\")\n",
    "\n",
    "### Conversation buffer memory is a type of temporary memory, and you can extract them in a variable.\n",
    "\n",
    "### Large Languge Models are \"stateless\"\n",
    "-- Each transaction is independent\n",
    "Chatbots appear to have memory by providing the full conversation as \"context\". The context is saved in a variable in the memory buffer.\n",
    "-- As the conversation goes long, the memory goes long too. The tokens become expensive. LangChain has different types of memories to solve these problems. One of them is we can import the ConversationBufferWindowMemory (that only keeps the memory within a window. k = 1, meaning last one converstaion's memory). Several of them are summarized below:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIoUJkbCJunV"
   },
   "source": [
    "# Memory Types\n",
    "\n",
    "### ConversationBufferMemory\n",
    "\n",
    "#### This memory allows for storying of messages and then extracts the messages in a variable.\n",
    "\n",
    "### ConversationBufferWindowMemory\n",
    "\n",
    "#### This memory keeps a list of the interactions of the conversation over time. It only uses the last k interactions.\n",
    "\n",
    "### ConversationTokenBufferMemory\n",
    "\n",
    "#### This memory keeps a buffer of recent interactions in memory, and uses token length rather than number of interactions to dertmine when to flush interactions.\n",
    "\n",
    "### ConversationSummaryMemory\n",
    "\n",
    "#### This memory creates a summary of the conversation over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HWO6gMVLp1F"
   },
   "source": [
    "# Additional Memory Types\n",
    "\n",
    "### Vector data memory\n",
    "\n",
    "#### Stores text (from conversation or elsewhere) in a vector database and retrieves the most relevant blocks of texts.\n",
    "\n",
    "### Entity memories\n",
    "#### using an LLM, it remebers details about specific entities.\n",
    "\n",
    "\n",
    "## We can use multiple memories at one time. E.g., Conversation memory + Entity memory to recall individuals.\n",
    "\n",
    "### You can also store the conversation in a conventional database (such as key-value store or SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II90hGrL6Ad4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_X2qcHz4Cgz",
    "outputId": "601b2335-e0b9-4199-e554-59575b3f78ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3cy5SgI5WhL",
    "outputId": "accba59c-a1c2-448d-a0ef-d50e33510f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, openai\n",
      "Successfully installed jiter-0.5.0 openai-1.42.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain_community\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xhGD3I7l5QWj"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ZQAMm6T33wBW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "66B87SOo4ArC"
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dEQs48XO6kBN"
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_APIKEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9K4u7Stw7Jmo"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT86w1mR72O3"
   },
   "source": [
    "Slightly different from Andrew's codes here: add api key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHfaMN3x5Og3",
    "outputId": "a4a2cbd8-e33c-419a-b65b-b000d7a4ce26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with?\n",
      "Hello, Alice! It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "# different llm has different ways of counting tokens, and generating contents, therefore we need to specify\n",
    "llm = ChatOpenAI(temperature=0.0, model_name=\"gpt-4\")\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    # verbose=True # the answer would be long and repetitive\n",
    "    verbose = False # the answer would be simple\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Hello, how can I assist you?\")\n",
    "print(response)\n",
    "\n",
    "response2 = conversation.predict(input=\"Hi, my name is Alice\")\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NGK9IOiYGUjr",
    "outputId": "913fdb2d-9fa3-4f92-acfb-b9ce77be5041"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Alice, as you just told me.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-YoBFEVLGbTd",
    "outputId": "ab86e5e7-1647-48bd-c03e-d6940f60aa1b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1 + 1 equals 2.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tF_S5GYFGruD",
    "outputId": "e018196e-ca79-473c-f487-c0b8ff8953ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Alice.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KXijnQpe5nD4",
    "outputId": "3c6cb6d0-9138-4c22-f62c-5a3d8071e8a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello again, Alice! How can I assist you further?'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "1ZUhKb2j8V1j",
    "outputId": "5ffee44a-9a25-4b5c-9846-117d67f97c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The sum of 1+1 is 2.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "I0fLcMbG8Xzn",
    "outputId": "97ab5249-3b09-4540-cfe7-fd5cd7954a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Alice.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "B8heXP_08uY_",
    "outputId": "1d8191e9-25f5-4b4c-ddc8-4471db592d01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: What was my last question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your last question was \"What is my name?\"'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was my last question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXHoDOAG8ayk",
    "outputId": "28c64c10-2b8c-4da5-eae6-349b07075ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: What was my last question?\n",
      "AI: Your last question was \"What is my name?\"\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AwSy1Eg8p5y",
    "outputId": "68670c73-df63-435c-cd04-78ee8c7a36d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hello, how can I assist you?\\nAI: Hello! As an AI, I don\\'t require assistance in the traditional sense. However, I\\'m here to help you with any questions or tasks you might have. Is there something specific you need help with today?\\nHuman: Hi, my name is Alice\\nAI: Hello, Alice! It\\'s nice to meet you. How can I assist you today?\\nHuman: Hi, my name is Alice\\nAI: Hello again, Alice! How can I assist you today?\\nHuman: What is 1+1?\\nAI: The sum of 1+1 is 2.\\nHuman: What is my name?\\nAI: Your name is Alice.\\nHuman: What was my last question?\\nAI: Your last question was \"What is my name?\"'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "eXIwgZk182MR"
   },
   "outputs": [],
   "source": [
    "# reset memory\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PNpatPao9Dnx",
    "outputId": "aa1e9e85-e6e0-4217-87e2-c8445fce1135"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "4n_LxU2j84WN"
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNE2tf1u86lH",
    "outputId": "de03ac32-ffe0-44cc-cd90-3c1ab782a313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asWqH2U_8_IO",
    "outputId": "e3678ad4-9226-4c6f-9a32-73dbf664d1da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Q_Dy7aTa9BGz"
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JRrT1if9Kv1",
    "outputId": "430afdac-4ebf-4934-d250-c67c0ed11612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6iwxao0-rkw"
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "na-LNQgj9MH-"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(l = 1)\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PreUR7k-DFNH",
    "outputId": "ee991b7f-6149-4f30-c21d-1718c81e8c95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, you see the short term memory:\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "m5bo_ML-DKZV",
    "outputId": "361e1623-32c4-4606-d6cc-eb45ee61f9f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: What was my last question?\n",
      "AI: Your last question was \"What is my name?\"\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello, Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: Hi, my name is Alice\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hello again, Alice! How can I assist you today?'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "9unCcqSGDMEI",
    "outputId": "5495cf36-b7b8-471b-870c-fd257efe6280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: What was my last question?\n",
      "AI: Your last question was \"What is my name?\"\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello, Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The sum of 1+1 is 2.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "30l-FsgXDRnd",
    "outputId": "536c846a-5ef1-4254-e6ec-56ef0f709769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello, how can I assist you?\n",
      "AI: Hello! As an AI, I don't require assistance in the traditional sense. However, I'm here to help you with any questions or tasks you might have. Is there something specific you need help with today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello, Alice! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: What was my last question?\n",
      "AI: Your last question was \"What is my name?\"\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello, Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alice.\n",
      "Human: Hi, my name is Alice\n",
      "AI: Hello again, Alice! How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The sum of 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Your name is Alice.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za1rF3y_HY_X"
   },
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "B18_XLShDThM"
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "BlTpJJmhHbOS"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "QIG6g9TwHdKP"
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyJMh5DdHfJf",
    "outputId": "139f2e30-ccba-48b2-f72e-cce3eb4cf124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfIqZUPvHky2"
   },
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "Qhfc79DlHhLy"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "udEhmfKoHo0L"
   },
   "outputs": [],
   "source": [
    "# create a long string, meaning a lot of tokens\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjpFMIM2HvNN",
    "outputId": "1acb641d-03bf-441e-b291-7ed26d485f4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool.'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "# only generate a summary of the previous saved conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "AlPQHuOIH3oM"
   },
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=1000)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L4iyI0_H5U5",
    "outputId": "658901ca-57aa-4280-db89-f1ad3a8fd9a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "\n",
    "# because the max_token_limit, it will generate  longer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "KpFhWDoLHw8T"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "q1MWu2XRH1Jc",
    "outputId": "045aeba6-d689-453b-f5c7-5e29609a5e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I would recommend showcasing the latest Language Model demo, as it is a cutting-edge technology that your customer will likely be very interested in. It demonstrates the power and capabilities of our AI technology in a tangible way. Additionally, you could also show some real-world examples of how the Language Model has been used successfully in various industries to solve complex problems.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEw-cGqXIEtN",
    "outputId": "f5c8fe88-a4b3-4627-de75-f7a5c81b5eb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI: I would recommend showcasing the latest Language Model demo, as it is a cutting-edge technology that your customer will likely be very interested in. It demonstrates the power and capabilities of our AI technology in a tangible way. Additionally, you could also show some real-world examples of how the Language Model has been used successfully in various industries to solve complex problems.\"}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSZEDk8gIKqb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
