{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0689733d",
   "metadata": {
    "id": "0689733d"
   },
   "source": [
    "# Retrieval\n",
    "\n",
    "Retrieval is the centerpiece of our retrieval augmented generation (RAG) flow.\n",
    "\n",
    "Let's get our vectorDB from before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LAdRu0OLLI8H",
   "metadata": {
    "id": "LAdRu0OLLI8H"
   },
   "source": [
    "# MMR algorithm (Maximum Marginal Relevance (MMR))\n",
    "\n",
    "## Query the Vector Store\n",
    "## Choose the \"fetch_k\" most similar responses\n",
    "## Within Those responses choose the \"k\" most diverse\n",
    "\n",
    "Motivation: You may not always want to choose the most similar responses. \\\n",
    "For example, when a chef is researching one kind of mushroom, he probably should care most if the mushroom is poisonous, besides which mushroom tastes best with certain food.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R0d-WF8qLMhb",
   "metadata": {
    "id": "R0d-WF8qLMhb"
   },
   "source": [
    "# LLM Aided Retrieval\n",
    "\n",
    "### There are several situations where the Query applied to teh DB is more than just the Question asked.\n",
    "### One is SelfQuery, where we use an LLM to convert the user question into a query.\n",
    "\n",
    "#### Question: What are some movies about aliens made in 1980?\n",
    "#### Query Parser, retrieve year, Subjects\n",
    "\n",
    "## Compression\n",
    "\n",
    "### Increase the number of results you can put the context by shriking the responses to only the relevant information.\n",
    "\n",
    "### Question --> Store --> Compression LLM --> LLM\n",
    "\n",
    "## Relevant splits, compressed relevant splits.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uNV9musMR7yH",
   "metadata": {
    "id": "uNV9musMR7yH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "F4tJYfBCLMBk",
   "metadata": {
    "id": "F4tJYfBCLMBk"
   },
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install Chroma\n",
    "# !pip install python-dotenv\n",
    "# !pip install chromadb\n",
    "# !pip install lark\n",
    "# !pip install langchain\n",
    "# !pip install langchain_community\n",
    "# !pip install chromadb\n",
    "# !pip install tiktoken\n",
    "# !pip install PdfReader\n",
    "# !pip install python-docx\n",
    "# !pip install pypdf\n",
    "# !pip install PyPDF2\n",
    "# !pip install pypdf\n",
    "# !pip install Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2569c6",
   "metadata": {
    "id": "ed2569c6"
   },
   "source": [
    "## Vectorstore retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "A3Ok7yZuSf2U",
   "metadata": {
    "id": "A3Ok7yZuSf2U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "OPENAI_API_KEY = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "t23UYXpdSE02",
   "metadata": {
    "id": "t23UYXpdSE02"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51b15e5c-9b92-4d40-a149-b56335d330d9",
   "metadata": {
    "height": 166,
    "id": "51b15e5c-9b92-4d40-a149-b56335d330d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "IEJ0jpsSLFpt",
   "metadata": {
    "id": "IEJ0jpsSLFpt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2d552e1",
   "metadata": {
    "id": "c2d552e1"
   },
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fe368042",
   "metadata": {
    "height": 64,
    "id": "fe368042",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# persist_directory = 'docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1vw2TVWr7Jey",
   "metadata": {
    "id": "1vw2TVWr7Jey"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "persist_directory = 'Embeddings'\n",
    "# persist_directory = 'sample_data/The_History_of_Starbucks.pdf'\n",
    "# embedding = OpenAIEmbeddings()\n",
    "embedding = OpenAIEmbeddings(\n",
    "    openai_api_key= OPENAI_API_KEY\n",
    ")\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "zaRjmw6W7x3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaRjmw6W7x3b",
    "outputId": "89d9e911-7a0b-48a2-f36f-d89ef9fc9a92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "pATiFvEB9Psk",
   "metadata": {
    "id": "pATiFvEB9Psk"
   },
   "outputs": [],
   "source": [
    "# in order to embedding texts from PDFs\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "o_mU8TKg83FL",
   "metadata": {
    "id": "o_mU8TKg83FL"
   },
   "outputs": [],
   "source": [
    "# pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "UXTJq_co85Ee",
   "metadata": {
    "id": "UXTJq_co85Ee"
   },
   "outputs": [],
   "source": [
    "# docs\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    text = ''\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c3J-7vyb87i7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3J-7vyb87i7",
    "outputId": "21975b3a-7953-4438-bd3a-47e5aad659b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path Embeddings/Pret.docx\n",
      "Processing DOCX: Pret.docx\n",
      "file_path Embeddings/Pret.pdf\n",
      "Processing PDF: Pret.pdf\n",
      "file_path Embeddings/chroma.sqlite3\n",
      "Skipping file: chroma.sqlite3\n",
      "file_path Embeddings/86f77b6f-4d5d-4d06-ab9f-8fa1cdb681f1\n",
      "Skipping file: 86f77b6f-4d5d-4d06-ab9f-8fa1cdb681f1\n",
      "file_path Embeddings/Starbucks.pdf\n",
      "Processing PDF: Starbucks.pdf\n",
      "vectordb._collection.count() count: 6\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for filename in os.listdir(persist_directory):\n",
    "    file_path = os.path.join(persist_directory, filename)\n",
    "    print(\"file_path\", file_path)\n",
    "\n",
    "    # process .pdf, .doc, .docx, files\n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Processing PDF: {filename}\")\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "    elif filename.endswith('.docx'):\n",
    "        print(f\"Processing DOCX: {filename}\")\n",
    "        text = extract_text_from_docx(file_path)\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "    elif filename.endswith('.doc'):\n",
    "        print(f\"Processing DOC: {filename}\")\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Skipping file: {filename}\")\n",
    "\n",
    "\n",
    "if not documents:\n",
    "    print(\"no documents found.\")\n",
    "else:\n",
    "    vectordb.add_texts(documents)\n",
    "    print(\"vectordb._collection.count() count:\", vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "deKGp_i99gTR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deKGp_i99gTR",
    "outputId": "85f87146-e9f6-48e5-cbbc-ffcf39428e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectordb)  # Yay!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "nza_IOC0JZyq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nza_IOC0JZyq",
    "outputId": "b4aaf5c8-e6d9-4f43-b8b3-ce71c1a9ba94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data', 'included'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectordb.metadata\n",
    "all_documents = vectordb.get()\n",
    "len(all_documents)\n",
    "all_documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "WzeHl5JsKDWz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzeHl5JsKDWz",
    "outputId": "77c60f20-f4f7-4311-af93-6db33e43c522"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['03ab5258-36ad-4e6f-8f2a-4c76e677de7a',\n",
       "  '1ce0fc9f-9721-4904-829f-899a4d1c718e',\n",
       "  '221d7043-b6a8-49bf-b99e-4e628b45ae8d',\n",
       "  '74ac0a29-b78b-4580-bf03-486cc03d13bd',\n",
       "  'b3f09b14-deb9-4ef6-ad33-9f17d49a7e63',\n",
       "  'c1b25563-cd62-4d48-be5c-630e770e854c'],\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents.get('ids'), all_documents.get('metadata'), all_documents.get('embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "NOCbZrrBKfMq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOCbZrrBKfMq",
    "outputId": "48209c9c-46fd-465b-f4cf-ef6cc1027841"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents.get('documents').__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8nuwm1NCKh-P",
   "metadata": {
    "id": "8nuwm1NCKh-P"
   },
   "outputs": [],
   "source": [
    "all_documents.get('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "xeMSe__yKkyl",
   "metadata": {
    "id": "xeMSe__yKkyl"
   },
   "outputs": [],
   "source": [
    "all_documents.get('includes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "udeoRmoQtT12",
   "metadata": {
    "id": "udeoRmoQtT12"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# # drive.mount('/content/drive')\n",
    "# persist_directory = 'Embeddings'\n",
    "# # persist_directory = 'sample_data/The_History_of_Starbucks.pdf'\n",
    "# # embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "tu9CHtm5vRD3",
   "metadata": {
    "id": "tu9CHtm5vRD3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# # drive.mount('/content/drive')\n",
    "# persist_directory = 'Embeddings'\n",
    "# # persist_directory = 'sample_data/The_History_of_Starbucks.pdf'\n",
    "# # embedding = OpenAIEmbeddings()\n",
    "# embedding = OpenAIEmbeddings(\n",
    "#     openai_api_key= OPENAI_API_KEY,\n",
    "# )\n",
    "# vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a0189dc5",
   "metadata": {
    "height": 98,
    "id": "a0189dc5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embedding = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)\n",
    "\n",
    "# vectordb = Chroma(\n",
    "#     persist_directory=persist_directory,\n",
    "#     embedding_function=embedding\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3659e0f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "3659e0f7",
    "outputId": "8b34cfc1-1588-4293-8fe9-3d7c83514455",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6qe78S-RJNnr",
   "metadata": {
    "id": "6qe78S-RJNnr"
   },
   "outputs": [],
   "source": [
    "# vectordb.similarity_search(\n",
    "#     query=\"Pret的创始日期是什么？\", k=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-82gmvw091qc",
   "metadata": {
    "id": "-82gmvw091qc"
   },
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "SEAlwZ2191GX",
   "metadata": {
    "id": "SEAlwZ2191GX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a807c758",
   "metadata": {
    "height": 98,
    "id": "a807c758",
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "    \"\"\"Today is a good day for a long walk.\"\"\",\n",
    "    \"\"\"One type of Italian dishes with Mushrooms is called La Cucina Italiana\"\"\",\n",
    "    \"\"\"Amanita phalloides is poisonous\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "715d54f3",
   "metadata": {
    "height": 30,
    "id": "715d54f3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "kr6XtI3EIID7",
   "metadata": {
    "id": "kr6XtI3EIID7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9a37b5a5",
   "metadata": {
    "height": 30,
    "id": "9a37b5a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies? Are they poisonous?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "24e3b025",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "24e3b025",
    "outputId": "56ae3e07-8999-4821-838b-843d4d91dd1f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.similarity_search(question, k=1)\n",
    "# It would still pick the one that is all-white, but would not be able to pick up the attributes\n",
    "# That are most important for chefs and customers, which is if they are poisonous.\n",
    "# this is one of the problem of pure similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4daa6c0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "4daa6c0d",
    "outputId": "53d909e7-6b3d-4014-e8db-292bd986880b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
       " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldb.max_marginal_relevance_search(question,k=4, fetch_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29e8c9",
   "metadata": {
    "id": "5a29e8c9"
   },
   "source": [
    "### Addressing Diversity: Maximum marginal relevance\n",
    "\n",
    "Last class we introduced one problem: how to enforce diversity in the search results.\n",
    "\n",
    "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9bb2c0a9",
   "metadata": {
    "height": 47,
    "id": "9bb2c0a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what did they say about the menu of Pret?\"\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f07f8793",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "height": 30,
    "id": "f07f8793",
    "outputId": "77a7a217-5dbb-4dc2-fe70-db69df3037ae",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'often\\nsimply\\nknown\\nas\\nPret,\\nis\\na\\nglobally\\nrecognized\\ncoffee\\nshop\\nand\\nsandwich\\nchain\\nwith\\na\\nunique\\napproach\\nto\\nfood\\nand\\nservice.\\nWith\\nits\\nroots\\nin\\nthe\\nbustling\\nstreets\\nof\\nLondon,\\nPret\\nhas\\ngrown\\nto\\nbecome\\na\\nfavorite\\namong\\nthose\\nseeking\\nfresh,\\nhealthy,\\nand\\nconvenient\\nmeals.\\nThis\\narticle\\ndelves\\ninto\\nthe\\nhistory\\nof\\nPret,\\ntracing\\nits\\njourney\\nfrom\\na\\nsingle\\nshop\\nto\\na\\nglobal\\nbrand.\\nThe\\nBeginning:\\nA\\nSimple\\nIdea\\nThe\\nstory\\nof\\nPret\\nA\\nManger\\nbegins\\nin\\n1983\\nwhen\\ntwo\\ncollege\\nfriends,\\nSinclair\\nBeecham\\nand\\nJulian\\nMetcalfe,\\nnoticed\\na\\ngap\\nin\\nthe\\nmarket\\nfor\\nfresh,\\nnatural\\nfood\\nthat\\ncould\\nbe\\nserved\\nquickly\\nto\\nbusy\\nLondoners.\\nInspired\\nby\\nthe\\nidea\\nof\\nproviding\\nan\\nalternative\\nto\\nthe\\nprocessed\\nand\\nunhealthy\\nfast\\nfood\\noptions\\nthat\\ndominated\\nthe\\nmarket,\\nthey\\ndecided\\nto\\ncreate\\na\\nplace\\nwhere\\npeople\\ncould\\nfind\\nfresh\\nsandwiches,\\nsalads,\\nand\\ncoffee\\nmade\\nfrom\\nhigh-quality\\ningredients.\\nThe\\nfirst\\nPret\\nA\\nManger\\nshop\\nopened\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[0].page_content[100:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e9f7e165",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "height": 30,
    "id": "e9f7e165",
    "outputId": "a9300776-6c9a-454f-88bf-539d20284958",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The\\nHistory\\nof\\nPret\\nA\\nManger:\\nFrom\\na\\nSingle\\nShop\\nto\\na\\nGlobal\\nPhenomenon\\nIntroduction\\nPret\\nA\\nManger,\\n'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "hwxAzW_xI-p8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hwxAzW_xI-p8",
    "outputId": "28e7bef4-e3e7-42fd-c147-0d261a4e846d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The History of Pret A Manger: From a Single Shop to a Global Phenomenon\\nIntroduction\\nPret A Manger, '"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss[2].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dccl-FXmLJ7r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dccl-FXmLJ7r",
    "outputId": "62a0ee6f-4f28-4401-88e9-e5c23dd8b1f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_ss == vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "Ei-DRsuRLRHr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ei-DRsuRLRHr",
    "outputId": "07025679-e4d7-489f-9d2e-32f5dcb224b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2VNWKDm6LXAl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VNWKDm6LXAl",
    "outputId": "114596f7-f28e-49d1-be3c-74dc14154a5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "type(docs_mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ca1b6",
   "metadata": {
    "id": "4c4ca1b6"
   },
   "source": [
    "Note the difference in results with `MMR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "222234c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "222234c5",
    "outputId": "ab323425-9fa7-42fb-abdc-c31f24609661",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    }
   ],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "93b20226",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "height": 30,
    "id": "93b20226",
    "outputId": "6567d523-d5b6-4fab-ae00-9a6a1b03b7d9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The\\nHistory\\nof\\nPret\\nA\\nManger:\\nFrom\\na\\nSingle\\nShop\\nto\\na\\nGlobal\\nPhenomenon\\nIntroduction\\nPret\\nA\\nManger,\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "17d39762",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "height": 30,
    "id": "17d39762",
    "outputId": "2191dd45-6734-4dec-b1fe-2f91d7ba3c0a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The History of Pret A Manger: From a Single Shop to a Global Phenomenon\\nIntroduction\\nPret A Manger, '"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b909bc",
   "metadata": {
    "id": "b2b909bc"
   },
   "source": [
    "### Addressing Specificity: working with metadata\n",
    "\n",
    "In last lecture, we showed that a question about the third lecture can include results from other lectures as well.\n",
    "\n",
    "To address this, many vectorstores support operations on `metadata`.\n",
    "\n",
    "`metadata` provides context for each embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3c1a60b2",
   "metadata": {
    "height": 30,
    "id": "3c1a60b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a8612840",
   "metadata": {
    "height": 98,
    "id": "a8612840",
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "97031876",
   "metadata": {
    "height": 47,
    "id": "97031876",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2708f6ae",
   "metadata": {
    "height": 30,
    "id": "2708f6ae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccc2d784",
   "metadata": {
    "id": "ccc2d784"
   },
   "source": [
    "### Addressing Specificity: working with metadata using self-query retriever\n",
    "\n",
    "But we have an interesting challenge: we often want to infer the metadata from the query itself.\n",
    "\n",
    "To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    "\n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b1d06084",
   "metadata": {
    "height": 64,
    "id": "b1d06084",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0aa5e698",
   "metadata": {
    "height": 217,
    "id": "0aa5e698",
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e143f05-908f-463d-a9de-408526c3947f",
   "metadata": {
    "id": "0e143f05-908f-463d-a9de-408526c3947f",
    "tags": []
   },
   "source": [
    "**Note:** The default model for `OpenAI` (\"from langchain.llms import OpenAI\") is `text-davinci-003`. Due to the deprication of OpenAI's model `text-davinci-003` on 4 January 2024, you'll be using OpenAI's recommended replacement model `gpt-3.5-turbo-instruct` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fc9de693-7bdb-463e-b124-9f72163b0bd8",
   "metadata": {
    "height": 166,
    "id": "fc9de693-7bdb-463e-b124-9f72163b0bd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_content_description = \"Cafe Inqury\"\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0, openai_api_key = OPENAI_API_KEY)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8K-zUt9-MMD_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8K-zUt9-MMD_",
    "outputId": "59bc2349-dec3-4579-8487-5119d9f78a24"
   },
   "outputs": [],
   "source": [
    "vectordb.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "79d781b9",
   "metadata": {
    "height": 30,
    "id": "79d781b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What did they say about the cafe Starbucks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "Wa0zCMC2Mes0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wa0zCMC2Mes0",
    "outputId": "153bf614-2f5b-4ff1-e466-6ba1838d6a19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.get(ids=[\"id1\", \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9BsB8wDOMby3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BsB8wDOMby3",
    "outputId": "27ca4fab-2574-46f5-c01d-42d4c32f79ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [], 'embeddings': None, 'metadatas': [], 'documents': [], 'uris': None, 'data': None, 'included': ['metadatas', 'documents']}\n"
     ]
    }
   ],
   "source": [
    "document = vectordb.get(ids=[\"id1\", \"id2\"])\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51778b0-1fcd-40a4-bd6b-0f13fec8acb1",
   "metadata": {
    "id": "c51778b0-1fcd-40a4-bd6b-0f13fec8acb1"
   },
   "source": [
    "**You will receive a warning** about predict_and_parse being deprecated the first time you executing the next line. This can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1d4f9f7d",
   "metadata": {
    "height": 30,
    "id": "1d4f9f7d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "db04374e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "db04374e",
    "outputId": "dcd0973b-f229-41bf-d400-aa70dbed4f9d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b8168",
   "metadata": {
    "id": "297b8168"
   },
   "source": [
    "### Additional tricks: compression\n",
    "\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
    "\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "\n",
    "Contextual compression is meant to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a060cf74",
   "metadata": {
    "height": 47,
    "id": "a060cf74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "038649c8",
   "metadata": {
    "height": 64,
    "id": "038649c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fc686cf2",
   "metadata": {
    "height": 64,
    "id": "fc686cf2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\", openai_api_key = OPENAI_API_KEY)\n",
    "compressor = LLMChainExtractor.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "82794397",
   "metadata": {
    "height": 81,
    "id": "82794397",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cde86848",
   "metadata": {
    "height": 64,
    "id": "cde86848",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"what did they say about matlab?\"\n",
    "# compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "# pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4fc4d",
   "metadata": {
    "id": "82c4fc4d"
   },
   "source": [
    "## Combining various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "161ae1ad",
   "metadata": {
    "height": 81,
    "id": "161ae1ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e77ccae1",
   "metadata": {
    "height": 64,
    "id": "e77ccae1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"what did they say about matlab?\"\n",
    "# compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "# pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b63e1",
   "metadata": {
    "id": "6c2b63e1"
   },
   "source": [
    "## Other types of retrieval\n",
    "\n",
    "It's worth noting that vectordb as not the only kind of tool to retrieve documents.\n",
    "\n",
    "The `LangChain` retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "83d2e808",
   "metadata": {
    "height": 81,
    "id": "83d2e808",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "Gp4zWq5mM1kA",
   "metadata": {
    "id": "Gp4zWq5mM1kA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bcf5b760",
   "metadata": {
    "height": 183,
    "id": "bcf5b760",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "loader = PyPDFLoader(\"Embeddings/Pret.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9bb0d781",
   "metadata": {
    "height": 64,
    "id": "9bb0d781",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0b1cc35f",
   "metadata": {
    "height": 64,
    "id": "0b1cc35f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What do people say about Pret?\"\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "docs_svm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2a1659c0",
   "metadata": {
    "height": 64,
    "id": "2a1659c0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "docs_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885389e",
   "metadata": {
    "height": 30,
    "id": "7885389e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
