{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088542f0",
   "metadata": {
    "id": "088542f0"
   },
   "source": [
    "# Lesson 2: Datasets For Reinforcement Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08cfaf3",
   "metadata": {
    "id": "e08cfaf3"
   },
   "source": [
    "### Loading and exploring the datasets\n",
    "\n",
    "\"Reinforcement Learning from Human Feedback\" **(RLHF)** requires the following datasets:\n",
    "- Preference dataset\n",
    "  - Input prompt, candidate response 0, candidate response 1, choice (candidate 0 or 1)\n",
    "- Prompt dataset\n",
    "  - Input prompt only, no response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1caf0",
   "metadata": {},
   "source": [
    "# Conceptual summary: \n",
    "\n",
    "- input text: ..... long text\n",
    "- summary: a short summary\n",
    "\n",
    "    - The training dataset would be: {input text, summary} pairs. But there is no correct to summarize the texts. There are many valid summaries we can write. The preferences are hard to quantify. \n",
    "\n",
    "    - Introduce supervised finue tuning: {input text, summary}\n",
    "\n",
    "    - RLHF: the training dataset would be: {input text, summary 1, summary 2, human preference.} human preference is labeled by humans, like which summary they like better... time/hr-consuming. \n",
    "    \n",
    "Three stages: \n",
    "    1. preference dataset --> \n",
    "    2. training (supervised) --> \n",
    "    3. reward model (in an reinforcement learning training loop) to fine tune our \"base LLM\". \n",
    "  \n",
    "Stage 1: Preference dataset\n",
    "\n",
    "    Summarize the following text: I want to satrt gardening but I don't ... --> BASE LLM --> Summary 1 (score 7), summary 2 (score 2)...\n",
    "\n",
    "    Scales like this are subjective and vary acorss people. Instead, one way of doing this, is to allow people to decide which summary they prefer and state the reasons behind their decisions. The dataset indicates one person's preferences, but not all human beings' preferences in general. \n",
    "\n",
    "    What is the point of tuning? more useful? less harmful contents? More positive? ... Need to define the goal first. \n",
    "\n",
    "\n",
    "Stage 2: Reward Model\n",
    "\n",
    "    Itself as another LLM (inference)\n",
    "    The input would be prompt, completion, and the output is the scalar value indicating how good the completion is. Eseentially, the reward model is a regression model that outputs numbers. \n",
    "    So one record of the preference dataset can be seen as {Prompt, winning candidate, losing candidate}. For each candidate completion, we get a score. While training, we are trying to minimize the reward model loss function. \n",
    "    \n",
    "Stage 3: Use RM in an RL loop to fine tune base llm. \n",
    "\n",
    "    During inference, we send the {prompt, completion} to a reward model to get a scalar indicating how good the completion is. The higher the number, the more likely the completion agrees with the human reviewer's assessment. \n",
    "    Use the reward modell in an RL loop to fine tune the base LLM. to produce the completion that maximize the scalar output of the reward model. Here we introduce a new dataset: prompt dataset: prompt1, prompt2, prompt3....\n",
    "    RL is useful when you want to train a model to learn how to solve a task that involves a complex and fairly open-ended objective. You may not know in advance what the optimal solution would be, but you can give the model rewards to guide it to an optimal solution. Formally, it teaches the agent to take actions that maximize the reward. The agent takes action that interacts with the environment and get a state that is different from the environment, and then get a reward that helps the agent learn the rules of the environment. (e.g. alphaGo)\n",
    "    A possible set of actions along with the probability that each action will lead to a higher reward. This set of actions is called a policy. The goal of RL is to learn a policy that can maximize the reward. The policy = \"The brain of the agent\". \n",
    "    How does RL relate to RLHF?: The policy is the base LLM that we want to tune. The current state is whatever is in the context (such as, the prompt and any generated text un to this point and actions are genertating tokens). Each time the base LLM outputs a completion, it receives a reward from the reward model indicating how aligned that generated text is. Learning the policy that maximizes the reward amounts to a LLM that produces completions with high scores from the reward model. In RLHF, the policy is learned via the policy gradient method, proximal policy optimization or PPO. This is a standard reinforcement learning algorithm.  \n",
    "    \n",
    "    \n",
    "We get a prompt from the prompt dataset, the BASE LLM compelete the prompt and send the completion and the prompt pair to the reward model, and then get a reward score. It then gets the model weights updated via PPO. and back to Base llm. In practice, you even add the penalty term to refrain the tuned model does not stray too far awar from teh base model. \n",
    "\n",
    "Source: Deep Reinforcement Learning from human preferences (Christiano etc)\n",
    "Training language models to follow instructions with human feedback. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9d650",
   "metadata": {},
   "source": [
    "### Full fine tuning\n",
    "\n",
    "- re-train the base model updating all of the model weights (parameters)\n",
    "\n",
    "### Parameter efficient fine tuning: \n",
    "- retrin the base model but only update a small subset of the model parameters\n",
    "- keep all the other parameters frozen. \n",
    "- deatiled methodologies are under research "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lbGW8HII9tWG",
   "metadata": {
    "id": "lbGW8HII9tWG"
   },
   "source": [
    "To summarize, the code loads two datasets (here preference and prompt), explores the structure of the preference data which contains prompts and their completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bUYi2W2N9s-L",
   "metadata": {
    "id": "bUYi2W2N9s-L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "236bce0e",
   "metadata": {
    "id": "236bce0e"
   },
   "source": [
    "#### Preference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3048b1b5-a01c-4c89-af5a-ce8f47d13218",
   "metadata": {
    "height": 30,
    "id": "3048b1b5-a01c-4c89-af5a-ce8f47d13218"
   },
   "outputs": [],
   "source": [
    "preference_dataset_path = 'sample_preference.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7822c372-b9d0-46c0-86c3-dc38508262e3",
   "metadata": {
    "height": 30,
    "id": "7822c372-b9d0-46c0-86c3-dc38508262e3"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb35514-6e2b-433c-a6f3-9411846fd2a6",
   "metadata": {
    "height": 30,
    "id": "5fb35514-6e2b-433c-a6f3-9411846fd2a6"
   },
   "outputs": [],
   "source": [
    "preference_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35bcc873-cd6a-4108-9c4a-47b7d27faab7",
   "metadata": {
    "height": 64,
    "id": "35bcc873-cd6a-4108-9c4a-47b7d27faab7"
   },
   "outputs": [],
   "source": [
    "with open(preference_dataset_path) as f:\n",
    "    for line in f:\n",
    "        preference_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2290d9",
   "metadata": {
    "id": "8b2290d9"
   },
   "source": [
    "- Print out to explore the preference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57964e85-d4bd-4b35-b437-e51a4493eb33",
   "metadata": {
    "height": 30,
    "id": "57964e85-d4bd-4b35-b437-e51a4493eb33"
   },
   "outputs": [],
   "source": [
    "sample_1 = preference_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9bff834-4c5e-46c4-8887-d3a29cc8de86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "e9bff834-4c5e-46c4-8887-d3a29cc8de86",
    "outputId": "3534c2b4-2414-437c-9f27-0e12e509fbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sample_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b473d0-bf39-4f8b-99e5-d6f795f33b83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "63b473d0-bf39-4f8b-99e5-d6f795f33b83",
    "outputId": "05e4af2e-9abb-49e1-b85b-8f528731663f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_text', 'candidate_0', 'candidate_1', 'choice'])\n"
     ]
    }
   ],
   "source": [
    "# This dictionary has four keys\n",
    "print(sample_1.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d524bb",
   "metadata": {
    "id": "21d524bb"
   },
   "source": [
    "- Key: 'input_test' is a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630d6337-0e6c-4ccb-987a-7d73de91c65a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "height": 30,
    "id": "630d6337-0e6c-4ccb-987a-7d73de91c65a",
    "outputId": "4a2ad3d3-04fb-4ac0-84a9-a09b11d7f352"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I live right next to a huge university, and have been applying for a variety of jobs with them through their faceless electronic jobs portal (the \"click here to apply for this job\" type thing) for a few months. \\n\\nThe very first job I applied for, I got an interview that went just so-so. But then, I never heard back (I even looked up the number of the person who called me and called her back, left a voicemail, never heard anything).\\n\\nNow, when I\\'m applying for subsequent jobs - is it that same HR person who is seeing all my applications?? Or are they forwarded to the specific departments?\\n\\nI\\'ve applied for five jobs there in the last four months, all the resumes and cover letters tailored for each open position. Is this hurting my chances? I never got another interview there, for any of the positions. [summary]: '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_1['input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb97174-bb2b-400a-b76a-81428063b76a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "height": 47,
    "id": "7eb97174-bb2b-400a-b76a-81428063b76a",
    "outputId": "cd802cd7-60a5-4009-e010-9dd4e65edecb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'plan something in those circumstances. [summary]: '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try with another examples from the list, and discover that all data end the same way\n",
    "preference_data[2]['input_text'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78232d32",
   "metadata": {
    "id": "78232d32"
   },
   "source": [
    "- Print 'candidate_0' and 'candidate_1', these are the completions for the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67ab282-3495-4aab-a43a-309978e03529",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "a67ab282-3495-4aab-a43a-309978e03529",
    "outputId": "53457d80-f0ac-4aa1-d6da-26236a1b6954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate_0:\n",
      " When applying through a massive job portal, is just one HR person seeing ALL of them?\n",
      "\n",
      "candidate_1:\n",
      " When applying to many jobs through a single university jobs portal, is just one HR person reading ALL my applications?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"candidate_0:\\n{sample_1.get('candidate_0')}\\n\")\n",
    "print(f\"candidate_1:\\n{sample_1.get('candidate_1')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3cc61",
   "metadata": {
    "id": "55d3cc61"
   },
   "source": [
    "- Print 'choice', this is the human labeler's preference for the results completions (candidate_0 and candidate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dae5b1cb-5411-462c-a122-bbb6264e4111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "dae5b1cb-5411-462c-a122-bbb6264e4111",
    "outputId": "af2818b9-cc15-4743-dbfa-02470d07edce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choice: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"choice: {sample_1.get('choice')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda6787",
   "metadata": {
    "id": "8eda6787"
   },
   "source": [
    "#### Prompt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c2785a-87df-4bc7-adb8-ccbdfff7b819",
   "metadata": {
    "height": 30,
    "id": "82c2785a-87df-4bc7-adb8-ccbdfff7b819"
   },
   "outputs": [],
   "source": [
    "prompt_dataset_path = 'sample_prompt.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3fd8ef-c9f3-4bc3-9404-cbe91fcae150",
   "metadata": {
    "height": 30,
    "id": "fa3fd8ef-c9f3-4bc3-9404-cbe91fcae150"
   },
   "outputs": [],
   "source": [
    "prompt_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e47624-472f-4f20-8f02-219b38e166ee",
   "metadata": {
    "height": 64,
    "id": "d0e47624-472f-4f20-8f02-219b38e166ee"
   },
   "outputs": [],
   "source": [
    "with open(prompt_dataset_path) as f:\n",
    "    for line in f:\n",
    "        prompt_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c90028-5da8-435d-a203-92a661154598",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "d1c90028-5da8-435d-a203-92a661154598",
    "outputId": "fedd4305-4ff6-44e7-b4df-160d0a56e94a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many prompts there are in this dataset\n",
    "len(prompt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08fd3a7",
   "metadata": {
    "id": "e08fd3a7"
   },
   "source": [
    "**Note**: It is important that the prompts in both datasets, the preference and the prompt, come from the same distribution.\n",
    "\n",
    "For this lesson, all the prompts come from the same dataset of [Reddit posts](https://github.com/openai/summarize-from-feedback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6da85e-5b6d-4d68-bb8e-70716205f28a",
   "metadata": {
    "height": 81,
    "id": "ff6da85e-5b6d-4d68-bb8e-70716205f28a"
   },
   "outputs": [],
   "source": [
    "# Function to print the information in the prompt dataset with a better visualization\n",
    "def print_d(d):\n",
    "    for key, val in d.items():\n",
    "        print(f\"key:{key}\\nval:{val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e665d332-38a2-4b8c-be5a-6e4c6c3392b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 30,
    "id": "e665d332-38a2-4b8c-be5a-6e4c6c3392b1",
    "outputId": "8abdfe65-71cb-4b59-c7a0-fb3e031d31df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:input_text\n",
      "val:I noticed this the very first day! I took a picture of it to send to one of my friends who is a fellow redditor. Later when I was getting to know my suitemates, I asked them if they ever used reddit, and they showed me the stencil they used to spray that! Along with the lion which is his trademark. \n",
      " But [summary]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_d(prompt_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9825d214-13a1-41c0-8280-b584d2f3bbd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "height": 47,
    "id": "9825d214-13a1-41c0-8280-b584d2f3bbd0",
    "outputId": "8c1cc860-f229-4470-a215-105e95200aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:input_text\n",
      "val:Nooooooo, I loved my health class! My teacher was amazing! Most days we just went outside and played and the facility allowed it because the health teacher's argument was that teens need to spend time outside everyday and he let us do that. The other days were spent inside with him teaching us how to live a healthy lifestyle. He had guest speakers come in and reach us about nutrition and our final was open book...if we even had a final.... [summary]: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with another prompt from the list\n",
    "print_d(prompt_data[1])\n",
    "\n",
    "# only one key, input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82a1d719-7c4b-4269-a3b7-d218ebfc3cf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "height": 30,
    "id": "82a1d719-7c4b-4269-a3b7-d218ebfc3cf2",
    "outputId": "27bcd461-e4dd-4b6e-ae4b-ead9acb61a03"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"mostly to places that I've never seen? [summary]: \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_data[1][\"input_text\"][-50:]\n",
    "# it always ends with '[summary]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3Wh1botN8Xqd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3Wh1botN8Xqd",
    "outputId": "dd7c6f70-9ab8-418b-ab18-2bd0942507e6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"ng to me from my childhood, or should I try to go mostly to places that I've never seen? [summary]: \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_data[1][\"input_text\"][-100:]\n",
    "# it always ends with '[summary]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ajOaBliU-0ks",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ajOaBliU-0ks",
    "outputId": "b92439a5-d249-4142-8f65-5d87381725c9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"s holiday)\\n\\nI feel like I'm going crazy trying to plan something in those circumstances. [summary]: \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preference_data[2][\"input_text\"][-100:]\n",
    "# it always ends with '[summary]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GyWKtqMH-rQN",
   "metadata": {
    "id": "GyWKtqMH-rQN"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FV6A3DTS-ply",
   "metadata": {
    "id": "FV6A3DTS-ply"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xf0PsniS9ZPW",
   "metadata": {
    "id": "xf0PsniS9ZPW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
